{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Datta0/notebooks/blob/main/nb/Qwen3_(4B)-GRPO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTBFAc5Mmxs4"
      },
      "source": [
        "To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n",
        "<div class=\"align-center\">\n",
        "<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n",
        "</div>\n",
        "\n",
        "To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n",
        "\n",
        "You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0xsm6BBmxs4"
      },
      "source": [
        "### News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hMEGvQBmxs5"
      },
      "source": [
        "**NEW** Unsloth now supports training the new **gpt-oss** model from OpenAI! You can start finetune gpt-oss for free with our **[Colab notebook](https://x.com/UnslothAI/status/1953896997867729075)**!\n",
        "\n",
        "Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).\n",
        "\n",
        "Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!\n",
        "\n",
        "Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNe95IAlmxs5"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VOQdkYQfmxs5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TMA5hZSJmxs5"
      },
      "outputs": [],
      "source": [
        "#@title Colab Extra Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkH_y8UC9lvv"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN75nmdx9lvw"
      },
      "source": [
        "Goal: To convert `Qwen3-4B-Base` into a reasoning model via GRPO by using OpenR1's Math dataset.\n",
        "\n",
        "We first pre fine-tune the model to make GRPO skip trying to match formatting - this speeds GRPO up."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_VLLM_STANDBY'] = '1'\n",
        "os.environ[\"UNSLOTH_ENABLE_LOGGING\"] = \"1\""
      ],
      "metadata": {
        "id": "kVAzlaPmn8D-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c58a41cfa9644288fe491207553d51b",
            "66c184cf05b548ed8a6e73e598ebc622",
            "d824bb0eca3e443fa5d0fdfb12a9dc29",
            "a838ff62428d478aad5c7a39886cea26",
            "ec3a12bc77fb47159c982e4212c31bf3",
            "92dadac30d0f4aa1989dd787b2e1eea1",
            "7695b5c82c2348c0bf4efd63cc781cdc",
            "2255ebe61a5f4a759a3ee52359c95f87",
            "1c95999c10ba4f5ab3401dfb3fb6764d",
            "88161b307fb448ed90905533ff1a9069",
            "442dce254cb34ad8a1bf21e74a464b3c",
            "59a7fd28ac8d45108fac4d589fd35b07",
            "72bd93276c904ad2bdcb985eae28b472",
            "9a3a27e099d04c0daa4c289f64fc11f2",
            "d3409807f3a14e49adc56e64a9fc5760",
            "847fb8ea629f494dbdab8174193723c0",
            "11281f513dc44571af9e0c2afc65154e",
            "ee41c56e13f74c04be6f17048b5f66e2",
            "83149feab93b4a348a263573be63ec53",
            "04d35183b4344d12a6d1c7748acbe3db",
            "cd416da687c24ba89f944d726b60dcda",
            "0e8d4212221542ee91422a40a804e529"
          ]
        },
        "id": "DkIvEkIIkEyB",
        "outputId": "3a2131a8-e201-4c49-eef7-84bc5d51ef1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched Gemma3Processor.__call__.\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched AutoHfQuantizer.merge_quantization_configs.\n",
            "ERROR:unsloth_zoo.temporary_patches.common:Unsloth: Skipped CsmDepthDecoderForCausalLM.forward\n",
            "Reason: New function removed output_attentions and output_hidden_states\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched CsmDepthDecoderForCausalLM.forward.\n",
            "ERROR:unsloth_zoo.temporary_patches.common:Unsloth: Skipped CsmForConditionalGeneration.forward\n",
            "Reason: New function removed output_attentions and output_hidden_states\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched CsmForConditionalGeneration.forward.\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched CsmForConditionalGeneration._merge_input_ids_with_input_values.\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched Gemma3nTextAltUp.predict.\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched Gemma3nTextAltUp.correct.\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched Gemma3nTextAltUp.scale_corrected_output.\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: Patched Gemma3nModel.get_placeholder_mask.\n",
            "ERROR:unsloth_zoo.temporary_patches.common:==================\n",
            "Failed to patch Please install triton_kernels. Error\n",
            "No module named 'triton_kernels'\n",
            "==================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 08-12 09:52:41 [importing.py:53] Triton module has been replaced with a placeholder.\n",
            "INFO 08-12 09:52:41 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n",
            "INFO:huggingface_hub.file_download:Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Patching vLLM v1 graph capture\n",
            "Unsloth: Patching vLLM v0 graph capture\n",
            "==((====))==  Unsloth 2025.8.4: Fast Qwen3 patching. Transformers: 4.55.0. vLLM: 0.8.5.post1.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "INFO:huggingface_hub.file_download:Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n",
            "INFO:huggingface_hub.file_download:Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "INFO:huggingface_hub.file_download:Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--colab/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n",
            "INFO:huggingface_hub.file_download:Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "INFO:huggingface_hub.file_download:Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n",
            "INFO:huggingface_hub.file_download:Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "INFO:huggingface_hub.file_download:Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--vram-16/blobs/c0fd5260818f672a9119328c53cc33df84f30698\n",
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa.incomplete'\n",
            "INFO:huggingface_hub.file_download:Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa\n",
            "INFO:huggingface_hub.file_download:Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa\n",
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa.incomplete'\n",
            "INFO:huggingface_hub.file_download:Downloading 'config.json' to '/root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa\n",
            "INFO:huggingface_hub.file_download:Download complete. Moving file to /root/.cache/huggingface/hub/models--unslothai--1/blobs/b45487c7fddc646b832426e98d1ba4da26a58eaa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: vLLM loading unsloth/Qwen3-4B-Base with actual GPU utilization = 69.34%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 160.\n",
            "Unsloth: vLLM's KV Cache can use up to 3.27 GB. Also swap space = 0 GB.\n",
            "WARNING 08-12 09:53:01 [config.py:2972] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 08-12 09:53:20 [config.py:717] This model supports multiple tasks: {'generate', 'reward', 'embed', 'score', 'classify'}. Defaulting to 'generate'.\n",
            "WARNING 08-12 09:53:20 [arg_utils.py:1658] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "INFO 08-12 09:53:20 [llm_engine.py:240] Initializing a V0 LLM engine (v0.8.5.post1) with config: model='unsloth/Qwen3-4B-Base', speculative_config=None, tokenizer='unsloth/Qwen3-4B-Base', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Qwen3-4B-Base, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"backend\":\"inductor\",\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"epilogue_fusion\":true,\"max_autotune\":false,\"shape_padding\":true,\"trace.enabled\":false,\"triton.cudagraphs\":true,\"debug\":false,\"dce\":true,\"memory_planning\":true,\"coordinate_descent_tuning\":true,\"trace.graph_diagram\":false,\"compile_threads\":4,\"group_fusion\":true,\"disable_progress\":false,\"verbose_progress\":true,\"triton.multi_kernel\":0,\"triton.use_block_ptr\":true,\"triton.enable_persistent_tma_matmul\":true,\"triton.autotune_at_compile_time\":false,\"triton.cooperative_reductions\":false,\"cuda.compile_opt_level\":\"-O2\",\"cuda.enable_cuda_lto\":true,\"combo_kernels\":false,\"benchmark_combo_kernel\":true,\"combo_kernel_foreach_dynamic_shapes\":true,\"enable_auto_functionalized_v2\":false},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n",
            "INFO 08-12 09:53:21 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 08-12 09:53:21 [cuda.py:289] Using XFormers backend.\n",
            "INFO 08-12 09:53:22 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
            "INFO 08-12 09:53:22 [model_runner.py:1108] Starting to load model unsloth/Qwen3-4B-Base...\n",
            "INFO 08-12 09:53:22 [weight_utils.py:265] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c58a41cfa9644288fe491207553d51b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 09:54:00 [loader.py:458] Loading weights took 37.97 seconds\n",
            "INFO 08-12 09:54:00 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 08-12 09:54:01 [model_runner.py:1140] Model loading took 7.6764 GiB and 38.798665 seconds\n",
            "INFO 08-12 09:54:11 [worker.py:287] Memory profiling takes 9.37 seconds\n",
            "INFO 08-12 09:54:11 [worker.py:287] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.69) = 10.22GiB\n",
            "INFO 08-12 09:54:11 [worker.py:287] model weights take 7.68GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.88GiB; the rest of the memory reserved for KV Cache is 1.64GiB.\n",
            "INFO 08-12 09:54:11 [executor_base.py:112] # cuda blocks: 745, # CPU blocks: 0\n",
            "INFO 08-12 09:54:11 [executor_base.py:117] Maximum concurrency for 2048 tokens per request: 5.82x\n",
            "INFO 08-12 09:54:11 [vllm_utils.py:669] Unsloth: Running patched vLLM v0 `capture_model`.\n",
            "INFO 08-12 09:54:11 [model_runner.py:1450] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Capturing CUDA graph shapes:   0%|          | 0/23 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59a7fd28ac8d45108fac4d589fd35b07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 09:54:28 [model_runner.py:1592] Graph capturing finished in 17 secs, took 0.34 GiB\n",
            "INFO 08-12 09:54:28 [vllm_utils.py:676] Unsloth: Patched vLLM v0 graph capture finished in 17 secs.\n",
            "INFO 08-12 09:54:30 [llm_engine.py:437] init engine (profile, create kv cache, warmup model) took 28.58 seconds\n",
            "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n",
            "Unsloth: Just some info: will skip parsing ['post_feedforward_layernorm', 'pre_feedforward_layernorm']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2025.8.4 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Can increase for longer reasoning traces\n",
        "lora_rank = 32 # Larger rank = smarter, but slower\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-4B-Base\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = False, # False for LoRA 16bit\n",
        "    fast_inference = True, # Enable vLLM fast inference\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.7, # Reduce if out of memory\n",
        "    unsloth_vllm_standby=True\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank*2, # *2 speeds up training\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9DuiVRLhMco"
      },
      "source": [
        "### GRPO chat template\n",
        "Since we're using a base model, we should set a chat template. You can make your own chat template as well!\n",
        "1. DeepSeek uses `<think>` and `</think>`, but this is **not** necessary - you can customize it however you like!\n",
        "2. A `system_prompt` is recommended to at least guide the model's responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "6UjowCbT-cFz",
        "outputId": "960172c5-ca7f-4054-8880-2ec63443dc52"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "reasoning_start = \"<start_working_out>\" # Acts as <think>\n",
        "reasoning_end   = \"<end_working_out>\"   # Acts as </think>\n",
        "solution_start  = \"<SOLUTION>\"\n",
        "solution_end    = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = \\\n",
        "f\"\"\"You are given a problem.\n",
        "Think about the problem and provide your working out.\n",
        "Place it between {reasoning_start} and {reasoning_end}.\n",
        "Then, provide your solution between {solution_start}{solution_end}\"\"\"\n",
        "system_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGgs0MJkDkYL"
      },
      "source": [
        "We create a simple chat template below. Notice `add_generation_prompt` includes prepending `<start_working_out>` to guide the model to start its reasoning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Y3fF9gMujY02"
      },
      "outputs": [],
      "source": [
        "chat_template = \\\n",
        "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
        "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages[1:] %}\"\\\n",
        "    \"{% else %}\"\\\n",
        "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
        "        \"{% set loop_messages = messages %}\"\\\n",
        "    \"{% endif %}\"\\\n",
        "    \"{% for message in loop_messages %}\"\\\n",
        "        \"{% if message['role'] == 'user' %}\"\\\n",
        "            \"{{ message['content'] }}\"\\\n",
        "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
        "            \"{{ message['content'] + eos_token }}\"\\\n",
        "        \"{% endif %}\"\\\n",
        "    \"{% endfor %}\"\\\n",
        "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
        "    \"{% endif %}\"\n",
        "\n",
        "# Replace with out specific template:\n",
        "chat_template = chat_template\\\n",
        "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
        "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEcLdymBEHdk"
      },
      "source": [
        "Let's see how our chat template behaves on an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "BciEDYSSYFNj",
        "outputId": "219a6bd1-957d-49cb-9160-b189e2b4a686"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>What is 1+1?<start_working_out>I think it's 2.<end_working_out><SOLUTION>2</SOLUTION><|endoftext|>What is 2+2?<start_working_out>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.apply_chat_template([\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 1+1?\"},\n",
        "    {\"role\" : \"assistant\", \"content\" : f\"{reasoning_start}I think it's 2.{reasoning_end}{solution_start}2{solution_end}\"},\n",
        "    {\"role\" : \"user\", \"content\" : \"What is 2+2?\"},\n",
        "], tokenize = False, add_generation_prompt = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mdsuGjxHrjT"
      },
      "source": [
        "### Pre fine-tuning for formatting\n",
        "We now use a subset of NVIDIA's [Open Math Reasoning dataset](https://huggingface.co/datasets/nvidia/OpenMathReasoning) which was filtered to only include high quality DeepSeek R1 traces.\n",
        "\n",
        "We'll only filter ~59 or so examples to first \"prime\" / pre fine-tune the model to understand our custom GRPO formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AXxM2lStVIkd",
        "outputId": "b3808ac3-51f2-4da8-b2ca-d5ccbfb4ee68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      expected_answer                                            problem  \\\n",
              "0                  14  Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...   \n",
              "6                  -2  Find the value of the parameter $a$ for which ...   \n",
              "9                  18  What is the sum of all real numbers $x$ for wh...   \n",
              "13                  2  Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...   \n",
              "17                 30  What is the largest positive integer that divi...   \n",
              "...               ...                                                ...   \n",
              "19243             244  Let \\( p \\), \\( q \\), and \\( r \\) be the disti...   \n",
              "19245               1  A bug is on the $0$ of a number line. At any p...   \n",
              "19247               4  A bus left point X for point Y. Two hours late...   \n",
              "19248              18  Each interior angle of a regular n-gon measure...   \n",
              "19250          0.8960  Find the probability that the second blue resu...   \n",
              "\n",
              "                                      generated_solution  \n",
              "0      <think>\\nOkay, let's see. I need to solve the ...  \n",
              "6      <think>\\nOkay, so I need to find the value of ...  \n",
              "9      <think>\\nOkay, so I need to solve the equation...  \n",
              "13     <think>\\nOkay, so I need to evaluate the infin...  \n",
              "17     <think>\\nAlright, so I need to find the larges...  \n",
              "...                                                  ...  \n",
              "19243  <think>\\nOkay, so I need to find the value of ...  \n",
              "19245  <think>\\nOkay, so I have this problem where a ...  \n",
              "19247  <think>\\nOkay, let's tackle this problem step ...  \n",
              "19248  <think>\\nOkay, let's see. I need to find the n...  \n",
              "19250  <think>\\nOkay, so I need to find the probabili...  \n",
              "\n",
              "[7507 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e79f8d47-49f5-4185-b8c2-90e6dbd47082\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>expected_answer</th>\n",
              "      <th>problem</th>\n",
              "      <th>generated_solution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14</td>\n",
              "      <td>Given $\\sqrt{x^2+165}-\\sqrt{x^2-52}=7$ and $x$...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's see. I need to solve the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-2</td>\n",
              "      <td>Find the value of the parameter $a$ for which ...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>18</td>\n",
              "      <td>What is the sum of all real numbers $x$ for wh...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to solve the equation...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>Evaluate the sum \\(\\sum_{n=1}^\\infty \\frac{\\ph...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to evaluate the infin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>30</td>\n",
              "      <td>What is the largest positive integer that divi...</td>\n",
              "      <td>&lt;think&gt;\\nAlright, so I need to find the larges...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19243</th>\n",
              "      <td>244</td>\n",
              "      <td>Let \\( p \\), \\( q \\), and \\( r \\) be the disti...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the value of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19245</th>\n",
              "      <td>1</td>\n",
              "      <td>A bug is on the $0$ of a number line. At any p...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I have this problem where a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19247</th>\n",
              "      <td>4</td>\n",
              "      <td>A bus left point X for point Y. Two hours late...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's tackle this problem step ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19248</th>\n",
              "      <td>18</td>\n",
              "      <td>Each interior angle of a regular n-gon measure...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, let's see. I need to find the n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19250</th>\n",
              "      <td>0.8960</td>\n",
              "      <td>Find the probability that the second blue resu...</td>\n",
              "      <td>&lt;think&gt;\\nOkay, so I need to find the probabili...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7507 rows √ó 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e79f8d47-49f5-4185-b8c2-90e6dbd47082')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e79f8d47-49f5-4185-b8c2-90e6dbd47082 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e79f8d47-49f5-4185-b8c2-90e6dbd47082');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-35bc3c18-235d-47ed-9eb2-f096e9b44602\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-35bc3c18-235d-47ed-9eb2-f096e9b44602')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-35bc3c18-235d-47ed-9eb2-f096e9b44602 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_664c065d-308f-4721-96ea-55de8b682358\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_664c065d-308f-4721-96ea-55de8b682358 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 7507,\n  \"fields\": [\n    {\n      \"column\": \"expected_answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 868,\n        \"samples\": [\n          \"672\",\n          \"335\",\n          \"575757\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"problem\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3895,\n        \"samples\": [\n          \"A club with 7 members forms three-person committees, but no two committees can have more than one member in common. What is the maximum number of committees that can be formed?\",\n          \"Find the smallest integer \\\\( a > 2 \\\\) such that \\\\( 2 \\\\mid a \\\\), \\\\( 3 \\\\mid (a+1) \\\\), \\\\( 4 \\\\mid (a+2) \\\\), \\\\( 5 \\\\mid (a+3) \\\\), and \\\\( 6 \\\\mid (a+4) \\\\).\",\n          \"Given the polynomial equation \\\\(x^3 - x = -1\\\\) with roots \\\\(a\\\\), \\\\(b\\\\), and \\\\(c\\\\), find the value of \\\\(\\\\frac{1}{1+a} + \\\\frac{1}{1+b} + \\\\frac{1}{1+c}\\\\).\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"generated_solution\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7507,\n        \"samples\": [\n          \"<think>\\nOkay, let's see. I need to solve this problem where there are three prime numbers p, q, and r. The equations given are pq + qr + rp = 191 and p + q = r - 1. The goal is to find p + q + r. Hmm, primes, so they must be 2 or odd primes. Let me start by analyzing the problem step by step.\\n\\nFirst, the equation p + q = r - 1. If I can express r in terms of p and q, maybe I can substitute that into the first equation. Let's try that. So, from the second equation, r = p + q + 1. That seems straightforward. Now, substituting this into the first equation, we get pq + qr + rp = 191. Let's replace r with (p + q + 1).\\n\\nSo, substituting, the first equation becomes:\\n\\npq + q(p + q + 1) + p(p + q + 1) = 191.\\n\\nLet me expand each term:\\n\\nFirst term: pq.\\n\\nSecond term: q*(p + q + 1) = pq + q\\u00b2 + q.\\n\\nThird term: p*(p + q + 1) = p\\u00b2 + pq + p.\\n\\nNow, adding all these together:\\n\\npq + (pq + q\\u00b2 + q) + (p\\u00b2 + pq + p) = 191.\\n\\nCombine like terms:\\n\\npq + pq + pq = 3pq.\\n\\nq\\u00b2 + p\\u00b2.\\n\\nq + p.\\n\\nSo altogether, the equation becomes:\\n\\np\\u00b2 + q\\u00b2 + 3pq + p + q = 191.\\n\\nHmm. Let me note that.\\n\\nAlternatively, maybe I can factor this expression. Let me see. Let's try to group terms. Wait, another approach: since r = p + q + 1, then p + q + r = (p + q) + (p + q + 1) = 2(p + q) + 1. So if I can find p + q, then multiplying by 2 and adding 1 gives me the answer. That's useful. So perhaps instead of dealing with the first equation directly, I can express it in terms of p + q.\\n\\nBut maybe let's work with the equation we derived: p\\u00b2 + q\\u00b2 + 3pq + p + q = 191.\\n\\nWait, another thought: p\\u00b2 + q\\u00b2 + 3pq. Let's recall that (p + q)^2 = p\\u00b2 + 2pq + q\\u00b2, so p\\u00b2 + q\\u00b2 + 3pq = (p + q)^2 + pq.\\n\\nTherefore, the equation can be rewritten as:\\n\\n(p + q)^2 + pq + p + q = 191.\\n\\nLet me set S = p + q, and P = pq. Then the equation becomes:\\n\\nS\\u00b2 + P + S = 191.\\n\\nBut from the second equation, we know that r = S + 1, so since we need to find S + r = S + (S + 1) = 2S + 1. So our target is 2S + 1. So if we can find S, we can find the answer.\\n\\nNow, the equation is S\\u00b2 + P + S = 191. But S and P are related as S = p + q, P = pq. For two primes p and q, their sum and product. Since p and q are primes, maybe we can list possible primes that add up to S and multiply to P. However, since S and P are variables here, maybe we can express P in terms of S from the equation.\\n\\nFrom S\\u00b2 + P + S = 191, so P = 191 - S\\u00b2 - S.\\n\\nTherefore, P = -S\\u00b2 - S + 191. But P = pq must be positive, so -S\\u00b2 - S + 191 > 0. Therefore, S\\u00b2 + S < 191. Let's see the possible values of S. Since S is the sum of two primes, which are at least 2 each, so S is at least 2 + 2 = 4. Also, since S\\u00b2 + S < 191, let's solve S\\u00b2 + S - 191 < 0. Let's find the roots of S\\u00b2 + S - 191 = 0.\\n\\nUsing quadratic formula: S = [-1 \\u00b1 sqrt(1 + 4*191)] / 2 = [-1 \\u00b1 sqrt(765)] / 2. sqrt(765) is approx 27.66, so S \\u2248 (-1 + 27.66)/2 \\u2248 13.33. So the positive root is approximately 13.33, so S must be less than 13.33. Therefore, S can be integers from 4 up to 13.\\n\\nSo possible S values: 4,5,6,7,8,9,10,11,12,13.\\n\\nBut since p and q are primes, their sum S must be even or odd. Since except for 2, all primes are odd. So if both p and q are odd primes, their sum is even. If one is 2 and the other is odd, then their sum is odd. So S can be even or odd. So possible values of S (from 4 to 13) can be checked.\\n\\nBut maybe instead of all possible S, let's check possible S values from 4 to 13 and see if for each S, P = 191 - S\\u00b2 - S, and check if P can be expressed as the product of two primes that add up to S.\\n\\nAlternatively, maybe first check which S gives P as a product of two primes.\\n\\nLet's start with S=4. Then P = 191 - 16 -4 = 171. Then check if 171 can be written as product of two primes that add up to 4. But 4 is the sum. The primes could be 2 and 2 (since 2+2=4). Then 2*2=4, but P here is 171. 4 \\u2260 171, so S=4 is invalid.\\n\\nNext S=5: P=191 -25 -5=161. 161 factors into 7*23. Check if 7 + 23 = 30, which is not 5. So that's not possible. Alternatively, are there primes adding to 5? 2 and 3, since 2+3=5. Then P=2*3=6. But 6 \\u2260 161. So S=5 invalid.\\n\\nS=6: P=191 -36 -6=149. 149 is a prime number, so can't be expressed as product of two primes. So invalid.\\n\\nS=7: P=191 -49 -7=135. 135 factors into 5*27 (but 27 not prime), 3*45, 9*15, none primes. So no. So invalid.\\n\\nS=8: P=191 -64 -8=119. 119 factors into 7*17. Check if 7+17=24\\u22608. Primes adding to 8 are 3+5=8 or 5+3. Then P=15. But 15\\u2260119. So invalid.\\n\\nS=9: P=191 -81 -9=101. 101 is prime, so no.\\n\\nS=10: P=191 -100 -10=81. 81=9*9, but 9 not prime. So no.\\n\\nS=11: P=191 -121 -11=59. 59 is prime. So no.\\n\\nS=12: P=191 -144 -12=35. 35=5*7. Check if 5 +7=12? Yes! 5 +7=12. So here, S=12, which is p + q=12, and pq=35. 5 and 7 are primes. So this works.\\n\\nSo then p=5 and q=7, or p=7 and q=5. Then r = S + 1 =12 +1=13. Check if r is prime: 13 is prime. So yes. So then p, q, r are 5,7,13 or 7,5,13. Then p + q + r =5 +7 +13=25. So 25 would be the answer.\\n\\nWait, let me check S=13 as well just to be thorough. S=13: P=191 -169 -13=9. 9=3*3. Check if 3 +3=6\\u226013. So no. So no.\\n\\nTherefore, only S=12 gives valid primes. So the answer is 25.\\n\\nLet me verify the original equations. pq + qr + rp. Let p=5, q=7, r=13. Then 5*7 +7*13 +13*5 =35 +91 +65=35+91=126+65=191. Which matches. And p + q =5 +7=12. r -1=13 -1=12. So that also matches. So it's correct. Therefore, the answer is 25.\\n</think>To solve the problem where three prime numbers \\\\( p, q, \\\\) and \\\\( r \\\\) satisfy the equations \\\\( pq + qr + rp = 191 \\\\) and \\\\( p + q = r - 1 \\\\), we proceed as follows:\\n\\n1. **Express \\\\( r \\\\) in terms of \\\\( p \\\\) and \\\\( q \\\\):**\\n   From the equation \\\\( p + q = r - 1 \\\\), we can solve for \\\\( r \\\\):\\n   \\\\[\\n   r = p + q + 1\\n   \\\\]\\n\\n2. **Substitute \\\\( r \\\\) into the first equation:**\\n   Substitute \\\\( r = p + q + 1 \\\\) into the equation \\\\( pq + qr + rp = 191 \\\\):\\n   \\\\[\\n   pq + q(p + q + 1) + p(p + q + 1) = 191\\n   \\\\]\\n   Expanding and combining like terms:\\n   \\\\[\\n   pq + pq + q^2 + q + p^2 + pq + p = 191\\n   \\\\]\\n   Simplify:\\n   \\\\[\\n   p^2 + q^2 + 3pq + p + q = 191\\n   \\\\]\\n\\n3. **Introduce new variables:**\\n   Let \\\\( S = p + q \\\\) and \\\\( P = pq \\\\). The equation becomes:\\n   \\\\[\\n   S^2 + P + S = 191\\n   \\\\]\\n\\n4. **Express \\\\( r \\\\) in terms of \\\\( S \\\\):**\\n   Since \\\\( r = p + q + 1 = S + 1 \\\\), we need to find \\\\( S \\\\) such that \\\\( S^2 + P + S = 191 \\\\) and \\\\( P = pq \\\\) is the product of two primes \\\\( p \\\\) and \\\\( q \\\\) that sum to \\\\( S \\\\).\\n\\n5. **Determine possible values for \\\\( S \\\\):**\\n   Solve the inequality \\\\( S^2 + S < 191 \\\\):\\n   \\\\[\\n   S^2 + S - 191 < 0\\n   \\\\]\\n   Using the quadratic formula \\\\( S = \\\\frac{-1 \\\\pm \\\\sqrt{1 + 4 \\\\cdot 191}}{2} \\\\):\\n   \\\\[\\n   S = \\\\frac{-1 \\\\pm \\\\sqrt{765}}{2}\\n   \\\\]\\n   Since \\\\( \\\\sqrt{765} \\\\approx 27.66 \\\\), we have:\\n   \\\\[\\n   S \\\\approx \\\\frac{-1 + 27.66}{2} \\\\approx 13.33\\n   \\\\]\\n   Therefore, \\\\( S \\\\) must be an integer between 4 and 13.\\n\\n6. **Check possible values of \\\\( S \\\\):**\\n   - For \\\\( S = 12 \\\\):\\n     \\\\[\\n     P = 191 - 12^2 - 12 = 191 - 144 - 12 = 35\\n     \\\\]\\n     Check if \\\\( 35 \\\\) can be written as the product of two primes that sum to 12:\\n     \\\\[\\n     35 = 5 \\\\times 7 \\\\quad \\\\text{and} \\\\quad 5 + 7 = 12\\n     \\\\]\\n     This works. So \\\\( p = 5 \\\\) and \\\\( q = 7 \\\\).\\n\\n7. **Find \\\\( r \\\\):**\\n   \\\\[\\n   r = S + 1 = 12 + 1 = 13\\n   \\\\]\\n\\n8. **Verify the solution:**\\n   - Check \\\\( pq + qr + rp = 191 \\\\):\\n     \\\\[\\n     5 \\\\times 7 + 7 \\\\times 13 + 13 \\\\times 5 = 35 + 91 + 65 = 191\\n     \\\\]\\n   - Check \\\\( p + q = r - 1 \\\\):\\n     \\\\[\\n     5 + 7 = 12 \\\\quad \\\\text{and} \\\\quad 13 - 1 = 12\\n     \\\\]\\n\\nSince all conditions are satisfied, the final answer is:\\n\\\\[\\n\\\\boxed{25}\\n\\\\]\",\n          \"<think>\\nOkay, let's see. I need to solve this problem where x and y are positive integers satisfying 2(x + y) = gcd(x, y) + lcm(x, y). And I have to find the ratio of the lcm to the gcd of x and y. Hmm, okay, let's break this down.\\n\\nFirst, I remember that for any two positive integers, the product of the lcm and gcd of those numbers is equal to the product of the numbers themselves. So, lcm(x, y) * gcd(x, y) = x * y. That might come in handy here. Let me note that down: lcm(x,y)*gcd(x,y) = x*y.\\n\\nGiven the equation 2(x + y) = gcd(x, y) + lcm(x, y), maybe I can express everything in terms of gcd and the ratio of x and y. Since gcd and lcm are involved, it might be helpful to let d = gcd(x, y), and then express x and y as x = d*a and y = d*b, where a and b are coprime integers (their gcd is 1). That's a standard approach for problems involving gcd and lcm.\\n\\nSo let me set d = gcd(x, y). Then x = d*a, y = d*b, with gcd(a, b) = 1. Then, the lcm(x, y) would be d*a*b, because lcm(x, y) = x*y / gcd(x, y) = (d*a*d*b)/d = d*a*b. Right, so lcm(x,y) = d*a*b.\\n\\nSubstituting these into the original equation: 2(x + y) = gcd(x, y) + lcm(x, y).\\n\\nSubstituting x = d*a, y = d*b, gcd = d, lcm = d*a*b. Then:\\n\\n2(d*a + d*b) = d + d*a*b.\\n\\nFactor out d from the left side: 2d(a + b) = d(1 + a*b).\\n\\nSince d is a positive integer, we can divide both sides by d, yielding:\\n\\n2(a + b) = 1 + a*b.\\n\\nSo now the equation simplifies to 2(a + b) = a*b + 1, where a and b are coprime positive integers. Hmm, okay. Now we have a simpler equation to solve: a*b - 2a - 2b + 1 = 0. Let me rearrange that:\\n\\na*b - 2a - 2b + 1 = 0.\\n\\nHmm, maybe factor this equation? Let me see. Adding 4 to both sides might help in factoring. Let's try:\\n\\na*b - 2a - 2b + 1 + 4 - 4 = 0\\n\\nSo, a*b - 2a - 2b + 4 = 3.\\n\\nWait, not sure. Alternatively, perhaps rearrange the terms:\\n\\na*b - 2a - 2b = -1.\\n\\nThen, add 4 to both sides:\\n\\na*b - 2a - 2b + 4 = 3.\\n\\nNow, left side can be factored as (a - 2)(b - 2) = 3. Because expanding (a - 2)(b - 2) gives a*b - 2a - 2b + 4. Yes, that's right. So:\\n\\n(a - 2)(b - 2) = 3.\\n\\nSince a and b are positive integers and coprime, we need to find pairs (a, b) such that their product is 3 when each is reduced by 2. Also, since a and b are coprime, (a - 2) and (b - 2) must be divisors of 3, which is prime. The positive divisors of 3 are 1 and 3.\\n\\nSo possible pairs (since a and b are positive integers, a - 2 and b - 2 must be at least such that a and b are positive. Let's see:\\n\\nCase 1: (a - 2) = 1 and (b - 2) = 3. Then, a = 3, b = 5. Check if gcd(a, b) = 1. gcd(3, 5) = 1, which is good.\\n\\nCase 2: (a - 2) = 3 and (b - 2) = 1. Then, a = 5, b = 3. Similarly, gcd(5, 3) = 1. So this is also valid.\\n\\nBut also, since 3 is prime, the only positive divisors are 1 and 3. But since we're considering positive integers, we could also consider if one of them is negative? But since a and b are positive, a - 2 and b - 2 must be positive or zero? Wait, but 3 is positive, so the factors must both be positive. Because if one of (a - 2) or (b - 2) were negative, their product would be negative, but 3 is positive. So both (a - 2) and (b - 2) must be positive. Thus, only the two cases above.\\n\\nAlternatively, maybe (a - 2) and (b - 2) could be 3 and 1 in some order, which gives the two cases. So the possible (a, b) are (3, 5) and (5, 3). Since a and b are interchangeable (since x and y are symmetric in the problem), these two cases would yield the same results.\\n\\nSo now, let's see. For (a, b) = (3, 5), then x = d*3, y = d*5. Similarly, for (a, b) = (5, 3), x = d*5, y = d*3. But since the problem is symmetric in x and y, both cases are equivalent.\\n\\nNow, since we need to find the ratio lcm(x, y)/gcd(x, y), let's compute that.\\n\\nRecall that lcm(x, y)/gcd(x, y) = (d*a*b)/d = a*b. So it's simply a*b. Since in both cases, a and b are 3 and 5, the product is 15. Therefore, the ratio is 15.\\n\\nWait, that seems too straightforward. Let me check.\\n\\nIf the ratio is a*b, then yes. Because lcm(x, y) is d*a*b and gcd(x, y) is d, so their ratio is (d*a*b)/d = a*b. Since a and b are 3 and 5, 3*5=15. Therefore, the answer is 15. So the answer is 15.\\n\\nBut let me verify with an example. Let's take d=1. Then x=3, y=5. Then gcd(3,5)=1, lcm=15. Then 2(x + y) = 2*(8) = 16. The right side is 1 + 15=16. So that works. If d=1, then 2(3+5)=16=1 +15.\\n\\nWhat if d=2? Then x=6, y=10. gcd(6,10)=2, lcm=30. Then 2(6 + 10)=2*16=32. The right side is 2 +30=32. So that also works. Then the ratio lcm/gcd is 30/2=15. So regardless of d, the ratio is always 15. Wait, because if x = d*a, y = d*b, then lcm(x,y)/gcd(x,y) = (d*a*b)/d = a*b, which is 15 as in the first case. So regardless of d, the ratio is always 15. Therefore, the answer is 15.\\n\\nSo even if d is some other positive integer, the ratio remains a*b =15. Therefore, the required ratio is 15.\\n\\nTherefore, the answer is 15. So \\\\boxed{15}.\\n\\n**Final Answer**\\n\\\\boxed{15}\\n</think>Given \\\\( x \\\\) and \\\\( y \\\\) are positive integers such that \\\\( 2(x + y) = \\\\gcd(x, y) + \\\\text{lcm}(x, y) \\\\), we need to find \\\\( \\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} \\\\).\\n\\nFirst, let \\\\( d = \\\\gcd(x, y) \\\\). Then, we can express \\\\( x \\\\) and \\\\( y \\\\) as \\\\( x = d \\\\cdot a \\\\) and \\\\( y = d \\\\cdot b \\\\), where \\\\( \\\\gcd(a, b) = 1 \\\\). The least common multiple (lcm) of \\\\( x \\\\) and \\\\( y \\\\) is given by \\\\( \\\\text{lcm}(x, y) = d \\\\cdot a \\\\cdot b \\\\).\\n\\nSubstituting these into the given equation:\\n\\\\[\\n2(d \\\\cdot a + d \\\\cdot b) = d + d \\\\cdot a \\\\cdot b\\n\\\\]\\nDividing both sides by \\\\( d \\\\):\\n\\\\[\\n2(a + b) = 1 + a \\\\cdot b\\n\\\\]\\nRearranging terms, we get:\\n\\\\[\\na \\\\cdot b - 2a - 2b + 1 = 0\\n\\\\]\\nAdding 4 to both sides to factorize:\\n\\\\[\\na \\\\cdot b - 2a - 2b + 4 = 3\\n\\\\]\\nThis can be factored as:\\n\\\\[\\n(a - 2)(b - 2) = 3\\n\\\\]\\nThe positive integer solutions for \\\\((a - 2)\\\\) and \\\\((b - 2)\\\\) are 1 and 3. Thus, the possible pairs \\\\((a, b)\\\\) are \\\\((3, 5)\\\\) and \\\\((5, 3)\\\\). Since \\\\( a \\\\) and \\\\( b \\\\) are coprime, both pairs are valid.\\n\\nThe ratio \\\\( \\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} \\\\) is given by:\\n\\\\[\\n\\\\frac{\\\\text{lcm}(x, y)}{\\\\gcd(x, y)} = \\\\frac{d \\\\cdot a \\\\cdot b}{d} = a \\\\cdot b\\n\\\\]\\nFor both pairs \\\\((3, 5)\\\\) and \\\\((5, 3)\\\\), the product \\\\( a \\\\cdot b = 15 \\\\).\\n\\nThus, the final answer is:\\n\\\\[\\n\\\\boxed{15}\\n\\\\]\",\n          \"<think>\\nOkay, so I need to find the remainder when the product of all odd numbers from 1 to 2005 is divided by 1000. Hmm, let's think about how to approach this. \\n\\nFirst, the product is 1 \\u00d7 3 \\u00d7 5 \\u00d7 ... \\u00d7 2005. That's a lot of numbers! Since we're dealing with division by 1000, maybe modular arithmetic can help here. The remainder when divided by 1000 is equivalent to the product modulo 1000. But calculating such a huge product directly seems impossible. There has to be a smarter way.\\n\\nI remember that when dealing with factorials and remainders, factors of 2 and 5 can create trailing zeros. However, here we're only multiplying odd numbers, so there are no factors of 2. But there might still be factors of 5. Wait, 1000 is 8\\u00d7125, which is 2^3 \\u00d7 5^3. Since the product is all odd numbers, it won't have factors of 2, but it can have factors of 5. Therefore, the product will be divisible by 5^3, but since there are no 2s, the product modulo 1000 might not be zero. Hmm, maybe I need to compute the product modulo 1000, but adjusting for the factors of 5?\\n\\nAlternatively, maybe split the problem into modulo 8 and modulo 125, then use the Chinese Remainder Theorem (CRT) to combine the results. Since 1000 = 8 \\u00d7 125, and 8 and 125 are coprime, CRT says that if I can find the remainder modulo 8 and modulo 125, then I can combine them to find the remainder modulo 1000. That might be a good approach.\\n\\nLet me start with modulo 8. The product is 1\\u00d73\\u00d75\\u00d77\\u00d79\\u00d7...\\u00d72005. But modulo 8, odd numbers repeat every 8 numbers. Let's see, the residues modulo 8 of odd numbers are 1,3,5,7,1,3,5,7,... So the pattern repeats every 4 terms. Wait, no. Wait, the numbers go 1,3,5,7,9\\u22611,11\\u22613,13\\u22615,15\\u22617, etc. So every 8 numbers, the cycle of residues 1,3,5,7 repeats twice. Wait, actually, modulo 8, the odd residues cycle every 4 numbers. Let's confirm:\\n\\n1 mod 8 =1\\n\\n3 mod8=3\\n\\n5 mod8=5\\n\\n7 mod8=7\\n\\n9 mod8=1\\n\\n11 mod8=3\\n\\n13 mod8=5\\n\\n15 mod8=7\\n\\nYes, every 4 terms, the cycle repeats. So how many terms are in the product 1\\u00d73\\u00d75\\u00d7...\\u00d72005? Let's find the number of terms first. The nth odd number is 2n-1. So 2n-1=2005 => n=(2005+1)/2=2006/2=1003. So there are 1003 terms.\\n\\nSo 1003 terms, each group of 4 terms (mod8) is 1\\u00d73\\u00d75\\u00d77=105. Then 105 mod8= 105 - 13\\u00d78=105-104=1. So each group of 4 terms multiplies to 1 mod8. Then how many full groups of 4 are there in 1003 terms? Let's divide 1003 by 4. 1003 \\u00f74=250.75. So 250 full groups, each contributing 1 mod8, and then a remainder of 3 terms. \\n\\nSo the total product modulo8 is (1^250) \\u00d7 (last three terms). The last three terms would be the terms after the 250th group. The 250th group ends at term 250\\u00d74=1000. So the 1001st term is 2\\u00d71001 -1=2001. Wait, no: the first term is 1=2\\u00d71-1, second term 3=2\\u00d72-1, so term k is 2k-1. Therefore, term 1001 is 2\\u00d71001 -1=2002-1=2001. Then the 1002nd term is 2003, 1003rd term is 2005. So the last three terms are 2001, 2003, 2005. Let's compute each mod8:\\n\\n2001 \\u00f78: 8\\u00d7250=2000, so 2001 mod8=1\\n\\n2003 mod8=3\\n\\n2005 mod8=5\\n\\nSo the last three terms modulo8 are 1\\u00d73\\u00d75=15 mod8=7.\\n\\nTherefore, total product mod8 is (1^250) \\u00d77=1\\u00d77=7 mod8.\\n\\nSo the remainder modulo8 is7.\\n\\nNow, we need to compute the product modulo125. This seems more complicated. Let's think.\\n\\nThe product is the product of all odd numbers from1 to2005. Wait, 2005=5\\u00d7401. So we can write the product as (1\\u00d73\\u00d75\\u00d77\\u00d7...\\u00d72005). Let's note that there are a lot of factors of 5 in this product, which would make the product divisible by 5 multiple times. However, modulo125 is 5^3, so if the product has at least three factors of 5, then modulo125 would be 0. Wait, but maybe even if it's divisible by 5^3, but we need to compute the actual remainder. Wait, but perhaps the product is divisible by 5^3, but when divided by 5^3, the remaining product modulo8 or something else. Wait, maybe not. Let me check how many factors of5 are in the product.\\n\\nThe number of factors of5 in the product:\\n\\nEach multiple of5 contributes at least one factor of5. Since we're dealing with odd numbers, the multiples of5 that are odd. So numbers divisible by5 but not by2. So numbers like5,15,25,...,2005. Let's count how many multiples of5 are in the product. The first term is5, which is5\\u00d71, then15=5\\u00d73,..., up to2005=5\\u00d7401. So the multiples of5 are5\\u00d7(1,3,5,...,401). Wait, 5\\u00d7k, where k is odd from1 to401. Because 5\\u00d7401=2005. So how many terms are there?\\n\\nThe number of terms k from1 to401 where k is odd. Since401 is odd, the number is (401 +1)/2=201. So there are201 multiples of5 in the product. Each contributes at least one factor of5. Additionally, multiples of25 contribute an extra factor of5. Similarly, multiples of125, 625, etc., contribute more factors.\\n\\nSo let's compute the total number of factors of5 in the product.\\n\\nNumber of multiples of5:201 (as above)\\n\\nNumber of multiples of25: These are numbers in the product divisible by25. Since the product includes numbers of the form5\\u00d7(odd numbers). So multiples of25 are numbers divisible by25, which are 25,75,125,...,2000. But 2005 is not divisible by25. Wait, wait, in the original product (all odd numbers up to2005), the multiples of25 must be odd multiples. So 25\\u00d71,25\\u00d73,..., up to the largest odd multiple less than or equal to2005.\\n\\n25\\u00d7k \\u22642005, where k is odd. Let's compute k_max:\\n\\n25k \\u22642005 => k \\u22642005/25=80.2. So the largest integer k is80, but since k has to be odd, the largest odd k is79. So 25\\u00d779=1975. Then 25\\u00d781=2025, which is over. So the multiples of25 in the product are25\\u00d71,25\\u00d73,...,25\\u00d779. Number of terms: (79-1)/2 +1=39 +1=40. Wait, from1 to79 odd numbers: number is (79+1)/2=40. So there are40 multiples of25.\\n\\nSimilarly, multiples of125:125\\u00d71,125\\u00d73,..., up to125\\u00d7k\\u22642005. 125\\u00d7k \\u22642005 =>k\\u226416.04. So k_max=15 (odd). So 125\\u00d715=1875. So multiples are125\\u00d71,125\\u00d73,...,125\\u00d715. Number of terms: (15-1)/2 +1=7+1=8.\\n\\nMultiples of625:625\\u00d71=625, next is625\\u00d73=1875, next is625\\u00d75=3125>2005. So only two multiples:625 and1875. But1875 is already counted as a multiple of125. So factors of625 contribute an extra factor of5 each. So number of multiples of625 is 2. 625 and1875.\\n\\nMultiples of3125:3125>2005, so none.\\n\\nSo total number of factors of5:\\n\\nFrom multiples of5:201\\n\\nFrom multiples of25:40 (each contributes an extra)\\n\\nFrom multiples of125:8 (each contributes another extra)\\n\\nFrom multiples of625:2 (each contributes another extra)\\n\\nTotal:201 +40 +8 +2=251.\\n\\nSo total factors of5 in the product:251.\\n\\nSimilarly, factors of2: since all numbers are odd, there are none. So the product is divisible by5^251 but not by2. So when dividing by5^3, since 251\\u22653, the product is divisible by5^3. Therefore, the product modulo125 is0? Wait, no, wait. Wait, 125 is5^3. If the product has at least three factors of5, then when divided by5^3, the quotient is an integer, but the remainder when divided by125 is0. So if the product is divisible by125, then the remainder is0. But the question is, when we divide the product by1000, which is8\\u00d7125, the remainder is to be found. However, we already considered that modulo8 is7, and modulo125 is... Hmm, but if modulo125 is0, then using CRT, we can say the remainder is a number congruent to7 mod8 and0 mod125. So we need to solve for x\\u22610 mod125 andx\\u22617 mod8.\\n\\nBut wait, let me confirm if modulo125 is indeed0. Let's check: since the product has at least three factors of5, then yes, the product is divisible by5^3, so product \\u22610 mod125. Therefore, modulo125 is0. Therefore, we have:\\n\\nx \\u22617 mod8\\n\\nx \\u22610 mod125\\n\\nWe need to find x such that x \\u22610 mod125 andx \\u22617 mod8. Let\\u2019s solve this system.\\n\\nLet x=125k. Then 125k\\u22617 mod8. Since125 mod8=5, so 5k\\u22617 mod8.\\n\\nWe solve 5k\\u22617 mod8.\\n\\nMultiply both sides by inverse of5 mod8. The inverse of5 mod8 is5, since5\\u00d75=25\\u22611 mod8. So multiply both sides by5:\\n\\nk\\u22617\\u00d75 mod8 =>k\\u226135 mod8 =>35\\u00f78=4*8=32, 35-32=3. So k\\u22613 mod8.\\n\\nTherefore, k=8m +3 for some integer m. Therefore, x=125(8m +3)=1000m +375. Therefore, the smallest non-negative solution is375. Therefore, the remainder is375 when divided by1000. So the answer is375.\\n\\nWait, but before accepting that, let me verify my steps again because this is tricky.\\n\\nFirst, confirming that the product is divisible by5^3: yes, since there are 251 factors of5, so 5^251 divides the product. Therefore, the product is divisible by5^3, so product\\u22610 mod125. That's correct.\\n\\nThen, solving x\\u22617 mod8 and x\\u22610 mod125. So x=125k. Then 125k\\u22615k mod8. So 5k\\u22617 mod8. Multiply both sides by inverse of5 mod8, which is5, since5\\u00d75=25\\u22611 mod8. So k\\u226135 mod8\\u22613 mod8. Therefore, k=8m +3, so x=125\\u00d73 +1000m=375 +1000m. So the minimal positive solution is375. So remainder is375 when divided by1000. That seems correct.\\n\\nBut wait, to make sure, let me check with an example. Let's take x=375. 375 \\u00f78=46*8=368, remainder7. So 375 mod8=7. 375 mod125=0. So yes, 375 satisfies both conditions. Therefore, the remainder is375. Therefore, the answer is\\\\boxed{375}.\\n\\nWait, but I need to make sure that my calculation for modulo8 and modulo125 are correct. Let me double-check the modulo8 calculation.\\n\\nEarlier, I considered that the product modulo8 is7. Let me recast that.\\n\\nThe product is1\\u00d73\\u00d75\\u00d77\\u00d7\\u2026\\u00d72005. When taking modulo8, each cycle of4 terms (1,3,5,7) multiplies to1\\u00d73\\u00d75\\u00d77=105\\u22611 mod8. Since 105/8=13\\u00d78=104, 105-104=1. Then, since there are1003 terms, how many cycles of4 are there? 1003 divided by4 is250 cycles with a remainder of3 terms. Then the product is (1)^250 \\u00d7 last3 terms.\\n\\nThe last3 terms are2001,2003,2005. 2001 mod8=1 (2001-8\\u00d7250=2001-2000=1), 2003=2001+2\\u21921+2=3 mod8, 2005=2001+4\\u21921+4=5 mod8. So last3 terms are1\\u00d73\\u00d75=15\\u22617 mod8. Therefore, total product\\u22611^250 \\u00d77\\u22617 mod8. Correct.\\n\\nTherefore, the logic holds. Therefore, the remainder is375. So I think that's the correct answer.\\n\\n**Final Answer**\\n\\\\boxed{375}\\n</think>To find the remainder when the product \\\\(1 \\\\times 3 \\\\times 5 \\\\times \\\\cdots \\\\times 2005\\\\) is divided by 1000, we use modular arithmetic and the Chinese Remainder Theorem (CRT).\\n\\n### Step 1: Calculate the product modulo 8\\n\\nThe sequence of odd numbers modulo 8 repeats every 4 terms: \\\\(1, 3, 5, 7\\\\). The product of each cycle is:\\n\\\\[\\n1 \\\\times 3 \\\\times 5 \\\\times 7 = 105 \\\\equiv 1 \\\\mod 8\\n\\\\]\\n\\nThere are 1003 terms in the product. Dividing 1003 by 4 gives 250 full cycles and a remainder of 3 terms. The remaining terms are 2001, 2003, and 2005. We calculate these modulo 8:\\n\\\\[\\n2001 \\\\equiv 1 \\\\mod 8, \\\\quad 2003 \\\\equiv 3 \\\\mod 8, \\\\quad 2005 \\\\equiv 5 \\\\mod 8\\n\\\\]\\nThe product of these remaining terms is:\\n\\\\[\\n1 \\\\times 3 \\\\times 5 = 15 \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\nThus, the product modulo 8 is:\\n\\\\[\\n1^{250} \\\\times 7 \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\n### Step 2: Calculate the product modulo 125\\n\\nWe need to determine the number of factors of 5 in the product. The sequence of odd numbers includes multiples of 5, 25, 125, and 625.\\n\\n- Multiples of 5: \\\\(5, 15, 25, \\\\ldots, 2005\\\\)\\n  - These are of the form \\\\(5 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 200\\\\)\\n  - Number of such terms: \\\\(\\\\frac{2005}{5} = 401\\\\), and half of these are odd, so \\\\(201\\\\) multiples of 5.\\n\\n- Multiples of 25: \\\\(25, 75, 125, \\\\ldots, 1975\\\\)\\n  - These are of the form \\\\(25 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 39\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1975}{25} = 79\\\\), and half of these are odd, so \\\\(40\\\\) multiples of 25.\\n\\n- Multiples of 125: \\\\(125, 375, 625, 875, 1125, 1375, 1625, 1875\\\\)\\n  - These are of the form \\\\(125 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1, 2, \\\\ldots, 15\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1875}{125} = 15\\\\), and half of these are odd, so \\\\(8\\\\) multiples of 125.\\n\\n- Multiples of 625: \\\\(625, 1875\\\\)\\n  - These are of the form \\\\(625 \\\\times (2k+1)\\\\) for \\\\(k = 0, 1\\\\)\\n  - Number of such terms: \\\\(\\\\frac{1875}{625} = 3\\\\), and half of these are odd, so \\\\(2\\\\) multiples of 625.\\n\\nTotal factors of 5:\\n\\\\[\\n201 + 40 + 8 + 2 = 251\\n\\\\]\\n\\nSince \\\\(251 \\\\geq 3\\\\), the product is divisible by \\\\(5^3 = 125\\\\). Therefore, the product modulo 125 is:\\n\\\\[\\n0 \\\\mod 125\\n\\\\]\\n\\n### Step 3: Combine results using the Chinese Remainder Theorem\\n\\nWe have:\\n\\\\[\\nx \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\\\[\\nx \\\\equiv 0 \\\\mod 125\\n\\\\]\\n\\nLet \\\\(x = 125k\\\\). Then:\\n\\\\[\\n125k \\\\equiv 7 \\\\mod 8\\n\\\\]\\nSince \\\\(125 \\\\equiv 5 \\\\mod 8\\\\), we have:\\n\\\\[\\n5k \\\\equiv 7 \\\\mod 8\\n\\\\]\\n\\nThe multiplicative inverse of 5 modulo 8 is 5, so:\\n\\\\[\\nk \\\\equiv 7 \\\\times 5 \\\\equiv 35 \\\\equiv 3 \\\\mod 8\\n\\\\]\\n\\nThus, \\\\(k = 8m + 3\\\\) for some integer \\\\(m\\\\). Therefore:\\n\\\\[\\nx = 125(8m + 3) = 1000m + 375\\n\\\\]\\n\\nThe smallest non-negative solution is:\\n\\\\[\\nx = 375\\n\\\\]\\n\\nTherefore, the remainder when the product is divided by 1000 is:\\n\\\\[\\n\\\\boxed{375}\\n\\\\]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "dataset = load_dataset(\"unsloth/OpenMathReasoning-mini\", split = \"cot\")\n",
        "dataset = dataset.to_pandas()[\n",
        "    [\"expected_answer\", \"problem\", \"generated_solution\"]\n",
        "]\n",
        "\n",
        "# Try converting to number - if not, replace with NaN\n",
        "is_number = pd.to_numeric(pd.Series(dataset[\"expected_answer\"]), errors = \"coerce\").notnull()\n",
        "# Select only numbers\n",
        "dataset = dataset.iloc[np.where(is_number)[0]]\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVRFqoSdIEVK"
      },
      "source": [
        "We have to format the dataset to follow our GRPO style formatting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Z9ydcV_Abfi6"
      },
      "outputs": [],
      "source": [
        "def format_dataset(x):\n",
        "    expected_answer = x[\"expected_answer\"]\n",
        "    problem = x[\"problem\"]\n",
        "\n",
        "    # Remove generated <think> and </think>\n",
        "    thoughts = x[\"generated_solution\"]\n",
        "    thoughts = thoughts.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
        "\n",
        "    # Strip newlines on left and right\n",
        "    thoughts = thoughts.strip()\n",
        "    # Add our custom formatting\n",
        "    final_prompt = \\\n",
        "        reasoning_start + thoughts + reasoning_end + \\\n",
        "        solution_start + expected_answer + solution_end\n",
        "    return [\n",
        "        {\"role\" : \"system\",    \"content\" : system_prompt},\n",
        "        {\"role\" : \"user\",      \"content\" : problem},\n",
        "        {\"role\" : \"assistant\", \"content\" : final_prompt},\n",
        "    ]\n",
        "\n",
        "dataset[\"Messages\"] = dataset.apply(format_dataset, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5NI47rOIRP2"
      },
      "source": [
        "Check to see if it worked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "LTdXBKcslhRH",
        "outputId": "a174ee90-e9fa-4ff1-b404-efde4f6016fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Given $\\\\sqrt{x^2+165}-\\\\sqrt{x^2-52}=7$ and $x$ is positive, find all possible values of $x$.<start_working_out>Okay, let's see. I need to solve the equation ‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7, and find all positive values of x. Hmm, radicals can be tricky, but maybe if I can eliminate the square roots by squaring both sides. Let me try that.\\n\\nFirst, let me write down the equation again to make sure I have it right:\\n\\n‚àö(x¬≤ + 165) - ‚àö(x¬≤ - 52) = 7.\\n\\nOkay, so the idea is to isolate one of the radicals and then square both sides. Let me try moving the second radical to the other side:\\n\\n‚àö(x¬≤ + 165) = 7 + ‚àö(x¬≤ - 52).\\n\\nNow, if I square both sides, maybe I can get rid of the square roots. Let's do that:\\n\\n(‚àö(x¬≤ + 165))¬≤ = (7 + ‚àö(x¬≤ - 52))¬≤.\\n\\nSimplifying the left side:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤.\\n\\nThe right side is expanded using the formula (a + b)¬≤ = a¬≤ + 2ab + b¬≤. So the right side becomes 7¬≤ + 2*7*‚àö(x¬≤ - 52) + (‚àö(x¬≤ - 52))¬≤, which is 49 + 14‚àö(x¬≤ - 52) + (x¬≤ - 52).\\n\\nSo putting it all together:\\n\\nx¬≤ + 165 = 49 + 14‚àö(x¬≤ - 52) + x¬≤ - 52.\\n\\nHmm, let's simplify the right side. The x¬≤ terms will cancel out, right? Let's subtract x¬≤ from both sides:\\n\\n165 = 49 + 14‚àö(x¬≤ - 52) - 52.\\n\\nSimplify the constants on the right:\\n\\n49 - 52 is -3, so:\\n\\n165 = -3 + 14‚àö(x¬≤ - 52).\\n\\nNow, add 3 to both sides to isolate the radical term:\\n\\n165 + 3 = 14‚àö(x¬≤ - 52).\\n\\nSo 168 = 14‚àö(x¬≤ - 52).\\n\\nDivide both sides by 14:\\n\\n168 / 14 = ‚àö(x¬≤ - 52).\\n\\n12 = ‚àö(x¬≤ - 52).\\n\\nNow, square both sides again to eliminate the square root:\\n\\n12¬≤ = x¬≤ - 52.\\n\\n144 = x¬≤ - 52.\\n\\nAdd 52 to both sides:\\n\\n144 + 52 = x¬≤.\\n\\n196 = x¬≤.\\n\\nSo x = ‚àö196 = 14.\\n\\nBut wait, since the problem states that x is positive, we only take the positive root. So x = 14.\\n\\nBut hold on, when dealing with squaring equations, sometimes extraneous solutions can come up. I should check if this solution actually satisfies the original equation.\\n\\nLet's plug x = 14 back into the original equation:\\n\\n‚àö(14¬≤ + 165) - ‚àö(14¬≤ - 52) = ?\\n\\nCalculate each term:\\n\\n14¬≤ is 196.\\n\\nSo first radical: ‚àö(196 + 165) = ‚àö361 = 19.\\n\\nSecond radical: ‚àö(196 - 52) = ‚àö144 = 12.\\n\\nSo 19 - 12 = 7, which is exactly the right-hand side. So yes, it checks out.\\n\\nTherefore, the only solution is x = 14. Since the problem says x is positive, we don't have to consider negative roots. So I think that's the answer.\\nTo solve the equation \\\\(\\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\\\) for positive \\\\(x\\\\), we proceed as follows:\\n\\n1. Start with the given equation:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} - \\\\sqrt{x^2 - 52} = 7\\n   \\\\]\\n\\n2. Isolate one of the square roots by moving \\\\(\\\\sqrt{x^2 - 52}\\\\) to the right side:\\n   \\\\[\\n   \\\\sqrt{x^2 + 165} = 7 + \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n3. Square both sides to eliminate the square root on the left:\\n   \\\\[\\n   (\\\\sqrt{x^2 + 165})^2 = (7 + \\\\sqrt{x^2 - 52})^2\\n   \\\\]\\n   Simplifying both sides, we get:\\n   \\\\[\\n   x^2 + 165 = 49 + 14\\\\sqrt{x^2 - 52} + (x^2 - 52)\\n   \\\\]\\n\\n4. Combine like terms on the right side:\\n   \\\\[\\n   x^2 + 165 = x^2 - 52 + 49 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n   Simplifying further:\\n   \\\\[\\n   x^2 + 165 = x^2 - 3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n5. Subtract \\\\(x^2\\\\) from both sides:\\n   \\\\[\\n   165 = -3 + 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n6. Add 3 to both sides to isolate the term with the square root:\\n   \\\\[\\n   168 = 14\\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n7. Divide both sides by 14:\\n   \\\\[\\n   12 = \\\\sqrt{x^2 - 52}\\n   \\\\]\\n\\n8. Square both sides again to eliminate the square root:\\n   \\\\[\\n   12^2 = x^2 - 52\\n   \\\\]\\n   Simplifying:\\n   \\\\[\\n   144 = x^2 - 52\\n   \\\\]\\n\\n9. Add 52 to both sides to solve for \\\\(x^2\\\\):\\n   \\\\[\\n   196 = x^2\\n   \\\\]\\n\\n10. Take the positive square root (since \\\\(x\\\\) is positive):\\n    \\\\[\\n    x = \\\\sqrt{196} = 14\\n    \\\\]\\n\\n11. Verify the solution by substituting \\\\(x = 14\\\\) back into the original equation:\\n    \\\\[\\n    \\\\sqrt{14^2 + 165} - \\\\sqrt{14^2 - 52} = \\\\sqrt{196 + 165} - \\\\sqrt{196 - 52} = \\\\sqrt{361} - \\\\sqrt{144} = 19 - 12 = 7\\n    \\\\]\\n    The solution checks out.\\n\\nThus, the only positive solution is:\\n\\\\[\\n\\\\boxed{14}\\n\\\\]<end_working_out><SOLUTION>14</SOLUTION><|endoftext|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokenizer.apply_chat_template(dataset[\"Messages\"][0], tokenize = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHV9BXYiIYaq"
      },
      "source": [
        "Let's truncate the pre fine-tuning dataset to `max_seq_length/2` since we don't want too long reasoning traces.\n",
        "\n",
        "Note this might take 2 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBHFlRbae9_s",
        "outputId": "03404583-a023-4b9d-d491-4604685b999d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset[\"N\"] = dataset[\"Messages\"].apply(lambda x: len(tokenizer.apply_chat_template(x)))\n",
        "\n",
        "dataset = dataset.loc[dataset[\"N\"] <= max_seq_length/2].copy()\n",
        "dataset.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6NkUCAGIj8N"
      },
      "source": [
        "We then tokenize the messages and convert it to a Hugging Face compatible dataset format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rgdtiV_f5hx",
        "outputId": "a1b11e44-9d13-4130-89d5-c0d55e44b64b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['expected_answer', 'problem', 'generated_solution', 'Messages', 'N', 'text', '__index_level_0__'],\n",
              "    num_rows: 59\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset[\"text\"] = tokenizer.apply_chat_template(dataset[\"Messages\"].values.tolist(), tokenize = False)\n",
        "dataset = Dataset.from_pandas(dataset)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAQJjQrYKzOk"
      },
      "source": [
        "Let's now pre fine-tune the model so it follows our custom GRPO formatting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "41033ac99c584df3b1709fe6d5f49931",
            "f8163e2ad75a49a1aff0fb3b51678a89",
            "d13156aa0c134cbf8ade77a6dfc0c522",
            "ddd758d7c4e3449480cce8dd717ef4e2",
            "1e5896313b854b94a94e939b637bc243",
            "a5e1287495354659afb8ceea3bb423ed",
            "a8d507942f4048c1b6c45aa4168a90f0",
            "80b296b2e4b744f598dec6ddbc368176",
            "85f525f32a9a465e8103df82a00aac51",
            "5347a8f55d3b49fd859dfaa94c25372e",
            "2a96d0ad9dd740ffbafd946782544a29"
          ]
        },
        "id": "woYi0SSygpqp",
        "outputId": "3b3863c9-a8fa-431e-e2ee-6838a8870b29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/59 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41033ac99c584df3b1709fe6d5f49931"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 1,\n",
        "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 2, # Set this for 1 full training run.\n",
        "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 5,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l4-2v_bLhZuE"
      },
      "outputs": [],
      "source": [
        "# trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRMBNUBgLC8T"
      },
      "source": [
        "Let's check if the model has learnt to follow the custom format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HJxrS76h3Ds",
        "outputId": "73337263-0c26-49eb-9a7a-2cc13eb79018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are given a problem.\n",
            "Think about the problem and provide your working out.\n",
            "Place it between <start_working_out> and <end_working_out>.\n",
            "Then, provide your solution between <SOLUTION></SOLUTION><|endoftext|>Jenifer has 82 cents in pennies and nickels. Her younger brother mistook all her nickels for dimes and counted the total as $1.47. How many pennies does Jenifer have?<start_working_out>Let's denote the number of nickels Jenifer has as N and the number of pennies as P.\n",
            "\n",
            "We know that each nickel is worth 5 cents and each penny is worth 1 cent. Jenifer has a total of 82 cents, so we can write the following equation:\n",
            "\n",
            "5N + P = 82\n",
            "\n",
            "Her younger brother mistook all her nickels for dimes, which are worth 10 cents each. He counted the total as $1.47, which is equivalent to 147 cents. So, we can write another equation:\n",
            "\n",
            "10N + P = 147\n",
            "\n",
            "Now we have a system of two equations with two variables:\n",
            "\n",
            "5N + P = 82\n",
            "10N + P = 147\n",
            "\n",
            "We can solve this system by subtracting the first equation from the second equation:\n",
            "\n",
            "(10N + P) - (5N + P) = 147 - 82\n",
            "5N = 65\n",
            "\n",
            "Now, we can solve for N:\n",
            "\n",
            "N = 65 / 5\n",
            "N = 13\n",
            "\n",
            "Now that we know Jenifer has 13 nickels, we can substitute this value back into the first equation to find the number of pennies:\n",
            "\n",
            "5(13) + P = 82\n",
            "65 + P = 82\n",
            "P = 82 - 65\n",
            "P = 17\n",
            "\n",
            "So, Jenifer has 17 pennies.<end_working_out>\n",
            "\n",
            "<SOLUTION>17</SOLUTION><|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    dataset[0][\"Messages\"][:2],\n",
        "    tokenize = False,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        ")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
        "    temperature = 0,\n",
        "    max_new_tokens = 1024,\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt = False),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtZ3qGOALF95"
      },
      "source": [
        "Yes it did follow the formatting! Great! Let's remove some items before the GRPO step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWSZ0DET7bob",
        "outputId": "e8a862c9-63ed-4158-ac1d-72a167b6e776"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "264"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "del dataset\n",
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KGgPgk_5S8r"
      },
      "source": [
        "### Data Prep\n",
        "<a name=\"Data\"></a>\n",
        "\n",
        "We're using Hugging Face's [Open R1 Math dataset](https://huggingface.co/datasets/open-r1/DAPO-Math-17k-Processed). You can also utilize OpenAI's famous [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7-eUrQn-OzE",
        "outputId": "e61ef3b2-ff67-4c7b-ffb8-22cf0f9107f4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info'],\n",
              "    num_rows: 14116\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"open-r1/DAPO-Math-17k-Processed\", \"en\", split = \"train\")\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b00gUsS-ROW"
      },
      "source": [
        "Let's look at the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "siopxjG8-ReF",
        "outputId": "67630298-d516-4760-ecf5-9539c20e197a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KGupRQqD-Wcf",
        "outputId": "5db12cba-87b5-4fef-f852-ec5dcd930091"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'34'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "dataset[0][\"solution\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmnXj6hn-Ydi"
      },
      "source": [
        "In GSM8K, ee notice all answers like about have a ####, so we extract it. But for the Open R1 dataset, we can skip the below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8JJGXKdJ-Zl_",
        "outputId": "02cee3a1-3b9c-42d2-c0b6-c106eff1b272"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'34'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "def extract_hash_answer(text):\n",
        "    # if \"####\" not in text: return None\n",
        "    # return text.split(\"####\")[1].strip()\n",
        "    return text\n",
        "extract_hash_answer(dataset[0][\"solution\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K30CygaU-dir"
      },
      "source": [
        "Let's map the dataset! and see the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyEVI972-d3n",
        "outputId": "09c226f8-c0d7-458b-c2a1-cdd90e5f0501"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': [{'content': 'You are given a problem.\\nThink about the problem and provide your working out.\\nPlace it between <start_working_out> and <end_working_out>.\\nThen, provide your solution between <SOLUTION></SOLUTION>',\n",
              "   'role': 'system'},\n",
              "  {'content': 'In triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.',\n",
              "   'role': 'user'}],\n",
              " 'solution': '34',\n",
              " 'data_source': 'math_dapo',\n",
              " 'source_prompt': [{'content': 'Solve the following math problem step by step. The last line of your response should be of the form Answer: $Answer (without quotes) where $Answer is the answer to the problem.\\n\\nIn triangle $ABC$, $\\\\sin \\\\angle A = \\\\frac{4}{5}$ and $\\\\angle A < 90^\\\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\\\angle BAD = \\\\angle DAC$ and $\\\\angle BDC = 90^\\\\circ$. Suppose that $AD = 1$ and that $\\\\frac{BD}{CD} = \\\\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\\\frac{a\\\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.\\n\\nRemember to put your answer on its own line after \"Answer:\".',\n",
              "   'role': 'user'}],\n",
              " 'ability': 'MATH',\n",
              " 'reward_model': {'ground_truth': '34', 'style': 'rule-lighteval/MATH_v2'},\n",
              " 'extra_info': {'index': '9a9b6eb4-a1cb-49d1-8c1e-62eaf2f74079'},\n",
              " 'answer': '34'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dataset = dataset.map(lambda x: {\n",
        "    \"prompt\" : [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\",   \"content\": x[\"prompt\"]},\n",
        "    ],\n",
        "    \"answer\": extract_hash_answer(x[\"solution\"]),\n",
        "})\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9m8eR9T-gMh"
      },
      "source": [
        "We create a regex format to match the reasoning sections and answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQwjTjNz-gY_",
        "outputId": "45b08719-b5be-4c7a-c48f-d4b690a724c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "re.compile(r'<end_working_out>.*?<SOLUTION>(.+?)</SOLUTION>[\\s]{0,}(?:<\\|endoftext\\|>)?[\\s]{0,}$',\n",
              "re.MULTILINE|re.DOTALL|re.UNICODE)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Add optional EOS token matching\n",
        "solution_end_regex = r\"</SOLUTION>[\\s]{0,}\" + \\\n",
        "    \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
        "\n",
        "match_format = re.compile(\n",
        "    rf\"{reasoning_end}.*?\"\\\n",
        "    rf\"{solution_start}(.+?){solution_end_regex}\"\\\n",
        "    rf\"[\\s]{{0,}}$\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "match_format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OycMneOq-iNC"
      },
      "source": [
        "We verify it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndzHnQ_6-jHt",
        "outputId": "0d99f80d-f44c-47b5-da2d-392c179a1f92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n2\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "match_format.findall(\n",
        "    \"Let me think!<end_working_out>\"\\\n",
        "    f\"<SOLUTION>\\n2\\n</SOLUTION>\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRMDAzDk2x6t",
        "outputId": "a1e4c6b8-ece8-4626-c794-e0f405640096"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  2  ']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "match_format.findall(\n",
        "    \"<start_working_out>Let me think!<end_working_out>\"\\\n",
        "    f\"<SOLUTION>  2  </SOLUTION>\\n\\n\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weOjmO5l-kl3"
      },
      "source": [
        "We now want to create a reward function to match the format exactly - we reward it with 3 points if it succeeds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qgFNXORy-lpO"
      },
      "outputs": [],
      "source": [
        "def match_format_exactly(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Match if format is seen exactly!\n",
        "        if match_format.search(response) is not None: score += 3.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf69i2WT-m4K"
      },
      "source": [
        "If it fails, we want to reward the model if it at least follows the format partially, by counting each symbol:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cUfHzCVx-nGK"
      },
      "outputs": [],
      "source": [
        "def match_format_approximately(completions, **kwargs):\n",
        "    scores = []\n",
        "    for completion in completions:\n",
        "        score = 0\n",
        "        response = completion[0][\"content\"]\n",
        "        # Count how many keywords are seen - we penalize if too many!\n",
        "        # If we see 1, then plus some points!\n",
        "\n",
        "        # No need to reward <start_working_out> since we always prepend it!\n",
        "        # score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
        "        score += 0.5 if response.count(reasoning_end)   == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_start)  == 1 else -1.0\n",
        "        score += 0.5 if response.count(solution_end)    == 1 else -1.0\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wAUWwtE-s6n"
      },
      "source": [
        "Finally, we want to extract the generated answer, and reward or penalize it! We also reward it based on how close the answer is to the true one via ratios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "hmtI_8gg-uIE"
      },
      "outputs": [],
      "source": [
        "def check_answer(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_format.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        score = 0\n",
        "        if guess is None:\n",
        "            scores.append(-2.0)\n",
        "            continue\n",
        "        # Correct answer gets 5 points!\n",
        "        if guess == true_answer:\n",
        "            score += 5.0\n",
        "        # Match if spaces are seen, but less reward\n",
        "        elif guess.strip() == true_answer.strip():\n",
        "            score += 3.5\n",
        "        else:\n",
        "            # We also reward it if the answer is close via ratios!\n",
        "            # Ie if the answer is within some range, reward it!\n",
        "            try:\n",
        "                ratio = float(guess) / float(true_answer)\n",
        "                if   ratio >= 0.9 and ratio <= 1.1: score += 2.0\n",
        "                elif ratio >= 0.8 and ratio <= 1.2: score += 1.5\n",
        "                else: score -= 2.5 # Penalize wrong answers\n",
        "            except:\n",
        "                score -= 4.5 # Penalize\n",
        "        scores.append(score)\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atMyfhXh-v3R"
      },
      "source": [
        "Also sometimes it might not be 1 number as the answer, but like a sentence for example \"The solution is $20\" -> we extract 20.\n",
        "\n",
        "We also remove possible commas for example as in 123,456"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVW0kL8q-wL5",
        "outputId": "320eafce-0127-4a52-c58b-eb805f292f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0.34']\n",
            "['123,456']\n",
            "['-0.234']\n",
            "['17']\n"
          ]
        }
      ],
      "source": [
        "match_numbers = re.compile(\n",
        "    solution_start + r\".*?[\\s]{0,}([-]?[\\d\\.\\,]{1,})\",\n",
        "    flags = re.MULTILINE | re.DOTALL\n",
        ")\n",
        "print(match_numbers.findall(\"<SOLUTION>  0.34  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>  123,456  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>  -0.234  </SOLUTION>\"))\n",
        "print(match_numbers.findall(\"<SOLUTION>17</SOLUTION>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbfaaAywNHHh"
      },
      "source": [
        "We now prepare our main function which will print out the generated responses and the true answer, along with another reward function which converts text to float via `float` and sees if it's the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GjBFrttr-y1_"
      },
      "outputs": [],
      "source": [
        "global PRINTED_TIMES\n",
        "PRINTED_TIMES = 0\n",
        "global PRINT_EVERY_STEPS\n",
        "PRINT_EVERY_STEPS = 5\n",
        "\n",
        "def check_numbers(prompts, completions, answer, **kwargs):\n",
        "    question = prompts[0][-1][\"content\"]\n",
        "    responses = [completion[0][\"content\"] for completion in completions]\n",
        "\n",
        "    extracted_responses = [\n",
        "        guess.group(1)\n",
        "        if (guess := match_numbers.search(r)) is not None else None \\\n",
        "        for r in responses\n",
        "    ]\n",
        "\n",
        "    scores = []\n",
        "    # Print only every few steps\n",
        "    global PRINTED_TIMES\n",
        "    global PRINT_EVERY_STEPS\n",
        "    if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:\n",
        "        print(\n",
        "            '*'*20 + f\"Question:\\n{question}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\"\n",
        "        )\n",
        "    PRINTED_TIMES += 1\n",
        "\n",
        "    for guess, true_answer in zip(extracted_responses, answer):\n",
        "        if guess is None:\n",
        "            scores.append(-2.5)\n",
        "            continue\n",
        "        # Convert to numbers\n",
        "        try:\n",
        "            true_answer = float(true_answer.strip())\n",
        "            # Remove commas like in 123,456\n",
        "            guess       = float(guess.strip().replace(\",\", \"\"))\n",
        "            scores.append(3.5 if guess == true_answer else -1.5)\n",
        "        except:\n",
        "            scores.append(0)\n",
        "            continue\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgOR3wJ_AyLr"
      },
      "source": [
        "Get the top 90% prompt length so we don't accidentally truncate them!\n",
        "\n",
        "Ie we'll remove the top 10% long prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EgAi4Q5fGE-",
        "outputId": "e02ba9b2-768d-493f-bb37-141facad6197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are given a problem.\n",
            "Think about the problem and provide your working out.\n",
            "Place it between <start_working_out> and <end_working_out>.\n",
            "Then, provide your solution between <SOLUTION></SOLUTION><|endoftext|>In triangle $ABC$, $\\sin \\angle A = \\frac{4}{5}$ and $\\angle A < 90^\\circ$. Let $D$ be a point outside triangle $ABC$ such that $\\angle BAD = \\angle DAC$ and $\\angle BDC = 90^\\circ$. Suppose that $AD = 1$ and that $\\frac{BD}{CD} = \\frac{3}{2}$. If $AB + AC$ can be expressed in the form $\\frac{a\\sqrt{b}}{c}$ where $a, b, c$ are pairwise relatively prime integers, find $a + b + c$.<start_working_out>\n",
            "Max Length =  201\n"
          ]
        }
      ],
      "source": [
        "tokenized = dataset.map(\n",
        "    lambda x: {\"tokens\" : tokenizer.apply_chat_template(x[\"prompt\"], add_generation_prompt = True, tokenize = True)},\n",
        "    batched = True,\n",
        ")\n",
        "print(tokenizer.decode(tokenized[0][\"tokens\"]))\n",
        "tokenized = tokenized.map(lambda x: {\"L\" : len(x[\"tokens\"])})\n",
        "\n",
        "import numpy as np\n",
        "maximum_length = int(np.quantile(tokenized[\"L\"], 0.9))\n",
        "print(\"Max Length = \", maximum_length)\n",
        "\n",
        "# Filter only samples smaller than 90% max length\n",
        "dataset = dataset.select(np.where(np.array(tokenized[\"L\"]) <= maximum_length)[0])\n",
        "del tokenized"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjwh0BBAqcQk",
        "outputId": "3b65b7c8-6913-4135-b255-6be253c3e6b7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['prompt', 'solution', 'data_source', 'source_prompt', 'ability', 'reward_model', 'extra_info', 'answer'],\n",
              "    num_rows: 12709\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IOMhVg-2AM"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptqkXK2D4d6p",
        "outputId": "c30944aa-5fb2-4819-9bc2-f9aac85bc556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: We now expect `per_device_train_batch_size` to be a multiple of `num_generations`.\n",
            "We will change the batch size of 1 to the `num_generations` of 4\n"
          ]
        }
      ],
      "source": [
        "max_prompt_length = maximum_length + 1 # + 1 just in case!\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "from vllm import SamplingParams\n",
        "vllm_sampling_params = SamplingParams(\n",
        "    min_p = 0.1,\n",
        "    top_p = 1.0,\n",
        "    top_k = -1,\n",
        "    seed = 3407,\n",
        "    stop = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")\n",
        "\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "training_args = GRPOConfig(\n",
        "    vllm_sampling_params = vllm_sampling_params,\n",
        "    temperature = 1.0,\n",
        "    learning_rate = 5e-6,\n",
        "    weight_decay = 0.01,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 2, # Increase to 4 for smoother training\n",
        "    num_generations = 4, # Decrease if out of memory\n",
        "    max_prompt_length = max_prompt_length,\n",
        "    max_completion_length = max_completion_length,\n",
        "    # num_train_epochs = 1, # Set to 1 for a full training run\n",
        "    max_steps = 100,\n",
        "    save_steps = 100,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # fp16_full_eval = True,\n",
        "    # per_device_eval_batch_size = 4,\n",
        "    # eval_accumulation_steps = 1,\n",
        "    # eval_strategy = \"steps\",\n",
        "    # eval_steps = 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Mv8UZO5hz-"
      },
      "source": [
        "And let's run the trainer! If you scroll up, you'll see a table of rewards. The goal is to see the `reward` column increase!\n",
        "\n",
        "You might have to wait 150 to 200 steps for any action. You'll probably get 0 reward for the first 100 steps. Please be patient!\n",
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vzOuSVCL_GA9",
        "outputId": "a3fc25cc-b463-4a8d-e5b4-91f80720552a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 12,709 | Num Epochs = 1 | Total steps = 100\n",
            "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 66,060,288 of 4,088,528,384 (1.62% trained)\n",
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING 08-12 09:57:35 [executor_base.py:215] Executor is not sleeping.\n",
            "********************Question:\n",
            "Compute the number of positive integers that divide at least two of the integers in the set $\\{1^1,2^2,3^3,4^4,5^5,6^6,7^7,8^8,9^9,10^{10}\\}$. \n",
            "Answer:\n",
            "22 \n",
            "Response:\n",
            " \n",
            "To solve this problem, we need to find the number of positive integers that are divisors of at least two numbers in the set $\\{1^1,2^2,3^3,4^4,5^5,6^6,7^7,8^8,9^9,10^{10}\\}$. Let's break down the problem step by step:\n",
            "\n",
            "1. **Prime Factorization:**\n",
            "   Each number in the set can be expressed in terms of its prime factors:\n",
            "   - $1^1$ has no prime factors other than 1.\n",
            "   - $2^2 = 2^2$\n",
            "   - $3^3 = 3^3$\n",
            "   - $4^4 = (2^2)^4 = 2^8$\n",
            "   - $5^5 = 5^5$\n",
            "   - $6^6 = (2 \\cdot 3)^6 = 2^6 \\cdot 3^6$\n",
            "   - $7^7 = 7^7$\n",
            "   - $8^8 = (2^3)^8 = 2^{24}$\n",
            "   - $9^9 = (3^2)^9 = 3^{18}$\n",
            "   - $10^{10} = (2 \\cdot 5)^{10} = 2^{10} \\cdot 5^{10}$\n",
            "\n",
            "2. **Identify Common Factors:**\n",
            "   We need to identify which positive integers can be divisors for at least two numbers in the set. These integers must be shared by at least two numbers.\n",
            "\n",
            "3. **Check for Divisibility:**\n",
            "   - For 1: Divides all numbers.\n",
            "   - For 2: Divides $2^2$, $2^8$, $2^6$, $2^{24}$, $2^{10}$.\n",
            "   - For 3: Divides $3^3$, $3^{18}$.\n",
            "   - For 5: Divides $5^5$, $5^{10}$.\n",
            "   - For 7: Divides $7^7$.\n",
            "   - For $2^1$ and $5^1$: Divides $10^{10}$.\n",
            "   - For $2^2$: Does not divide any other number.\n",
            "   - For $3^2$: Does not divide any other number.\n",
            "   - For $5^2$: Does not divide any other number.\n",
            "   - For $7^2$: Does not divide any other number.\n",
            "   And so on...\n",
            "\n",
            "4. **Counting the Positive Divisors:**\n",
            "   By examining the prime factorizations, we count the number of common divisors. The common divisors are $1, 2, 3, 5, 7$ and their combinations like $6, 10, 14, 15, 21, 35, 70, \\ldots$.\n",
            "\n",
            "</end_working_out><SOLUTION> There are a total of 17 positive integers that divide at least two of the integers in the given set. These integers include $1, 2, 3, 5, 7, 6, 10, 14, 15, 21, 35, 36, 45, 50, 63, 70, 91, 90, 126, 125, 150, 180, 189, 245, 250, 315, 360, 490, 525, 567, 630, 756, 784, 840, 945, 1050, 1092, 1125, 1225, 1260, 1344, 1400$.</SOLUTION> \n",
            "Extracted:\n",
            "17\n",
            "INFO 08-12 09:59:07 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 09:59:07 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 09:59:08 [worker.py:145] Sleep mode freed 1.84 GiB memory, 8.71 GiB memory is still in use.\n",
            "INFO 08-12 09:59:08 [executor_base.py:210] It took 0.851588 seconds to fall asleep.\n",
            "INFO 08-12 09:59:30 [executor_base.py:226] It took 0.897941 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 09:59:30 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 09:59:30 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 09:59:31 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.53 GiB memory is still in use.\n",
            "INFO 08-12 09:59:31 [executor_base.py:210] It took 0.735602 seconds to fall asleep.\n",
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 15/100 23:00 < 2:30:27, 0.01 it/s, Epoch 0.00/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completions / mean_length</th>\n",
              "      <th>completions / min_length</th>\n",
              "      <th>completions / max_length</th>\n",
              "      <th>completions / clipped_ratio</th>\n",
              "      <th>completions / mean_terminated_length</th>\n",
              "      <th>completions / min_terminated_length</th>\n",
              "      <th>completions / max_terminated_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>entropy</th>\n",
              "      <th>rewards / match_format_exactly / mean</th>\n",
              "      <th>rewards / match_format_exactly / std</th>\n",
              "      <th>rewards / match_format_approximately / mean</th>\n",
              "      <th>rewards / match_format_approximately / std</th>\n",
              "      <th>rewards / check_answer / mean</th>\n",
              "      <th>rewards / check_answer / std</th>\n",
              "      <th>rewards / check_numbers / mean</th>\n",
              "      <th>rewards / check_numbers / std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.437500</td>\n",
              "      <td>1.621503</td>\n",
              "      <td>640.750000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>1846.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>468.571442</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>854.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.312500</td>\n",
              "      <td>1.251784</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.125000</td>\n",
              "      <td>0.517549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-5.062500</td>\n",
              "      <td>2.460485</td>\n",
              "      <td>1244.625000</td>\n",
              "      <td>504.000000</td>\n",
              "      <td>1846.000000</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>1044.166748</td>\n",
              "      <td>504.000000</td>\n",
              "      <td>1495.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.060660</td>\n",
              "      <td>-1.312500</td>\n",
              "      <td>1.688987</td>\n",
              "      <td>-2.312500</td>\n",
              "      <td>0.883884</td>\n",
              "      <td>-1.812500</td>\n",
              "      <td>0.883884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.687500</td>\n",
              "      <td>6.199555</td>\n",
              "      <td>614.500000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>1703.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>614.500000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>1703.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>1.500000</td>\n",
              "      <td>1.603567</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>2.034304</td>\n",
              "      <td>-0.812500</td>\n",
              "      <td>3.261655</td>\n",
              "      <td>-0.187500</td>\n",
              "      <td>2.374530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-3.250000</td>\n",
              "      <td>2.065294</td>\n",
              "      <td>405.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>802.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>405.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>802.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>1.552648</td>\n",
              "      <td>-0.375000</td>\n",
              "      <td>1.552648</td>\n",
              "      <td>-2.687500</td>\n",
              "      <td>1.131923</td>\n",
              "      <td>-1.312500</td>\n",
              "      <td>0.530330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-4.125000</td>\n",
              "      <td>2.570111</td>\n",
              "      <td>727.875000</td>\n",
              "      <td>197.000000</td>\n",
              "      <td>1462.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>727.875000</td>\n",
              "      <td>197.000000</td>\n",
              "      <td>1462.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.060660</td>\n",
              "      <td>-1.312500</td>\n",
              "      <td>1.688987</td>\n",
              "      <td>-2.062500</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>-1.125000</td>\n",
              "      <td>1.922610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.937500</td>\n",
              "      <td>5.202440</td>\n",
              "      <td>636.875000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>1640.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>636.875000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>1640.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.060660</td>\n",
              "      <td>-0.750000</td>\n",
              "      <td>1.388730</td>\n",
              "      <td>-1.125000</td>\n",
              "      <td>2.474874</td>\n",
              "      <td>-0.437500</td>\n",
              "      <td>2.569568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.000000</td>\n",
              "      <td>6.165009</td>\n",
              "      <td>458.375000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>760.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>458.375000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>760.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>1.552648</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.792843</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>2.764572</td>\n",
              "      <td>-0.625000</td>\n",
              "      <td>2.587746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-3.625000</td>\n",
              "      <td>4.373638</td>\n",
              "      <td>834.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1846.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>690.000061</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1308.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.060660</td>\n",
              "      <td>-1.125000</td>\n",
              "      <td>1.747447</td>\n",
              "      <td>-2.312500</td>\n",
              "      <td>0.883884</td>\n",
              "      <td>-0.562500</td>\n",
              "      <td>2.651650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-4.000000</td>\n",
              "      <td>4.459733</td>\n",
              "      <td>455.250000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>904.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>455.250000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>904.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.388730</td>\n",
              "      <td>-1.125000</td>\n",
              "      <td>1.747447</td>\n",
              "      <td>-1.625000</td>\n",
              "      <td>1.274755</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.534522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.000000</td>\n",
              "      <td>2.732342</td>\n",
              "      <td>611.500000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>1669.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>611.500000</td>\n",
              "      <td>37.000000</td>\n",
              "      <td>1669.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>1.388730</td>\n",
              "      <td>-1.125000</td>\n",
              "      <td>1.922610</td>\n",
              "      <td>-2.375000</td>\n",
              "      <td>0.876275</td>\n",
              "      <td>-2.250000</td>\n",
              "      <td>0.462910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>-0.000000</td>\n",
              "      <td>-3.875000</td>\n",
              "      <td>2.404701</td>\n",
              "      <td>462.250000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>462.250000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>1030.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.375000</td>\n",
              "      <td>1.552648</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-1.500000</td>\n",
              "      <td>1.035098</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.500000</td>\n",
              "      <td>2.779331</td>\n",
              "      <td>556.375000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>1061.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>556.375000</td>\n",
              "      <td>250.000000</td>\n",
              "      <td>1061.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.060660</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.603567</td>\n",
              "      <td>-2.062500</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>-1.812500</td>\n",
              "      <td>0.883884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.750000</td>\n",
              "      <td>2.836944</td>\n",
              "      <td>660.250000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1846.000000</td>\n",
              "      <td>0.125000</td>\n",
              "      <td>490.857178</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>992.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>No Log</td>\n",
              "      <td>1.125000</td>\n",
              "      <td>1.552648</td>\n",
              "      <td>-0.937500</td>\n",
              "      <td>1.953705</td>\n",
              "      <td>-2.687500</td>\n",
              "      <td>1.131923</td>\n",
              "      <td>-1.250000</td>\n",
              "      <td>0.845154</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 09:59:49 [executor_base.py:226] It took 0.640196 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:01:27 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:01:27 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:01:28 [worker.py:145] Sleep mode freed 2.36 GiB memory, 8.88 GiB memory is still in use.\n",
            "INFO 08-12 10:01:28 [executor_base.py:210] It took 0.642469 seconds to fall asleep.\n",
            "INFO 08-12 10:01:51 [executor_base.py:226] It took 0.911090 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:01:51 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:01:51 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:01:52 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.76 GiB memory is still in use.\n",
            "INFO 08-12 10:01:52 [executor_base.py:210] It took 0.630025 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:02:10 [executor_base.py:226] It took 0.714442 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:03:34 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:03:34 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:03:34 [worker.py:145] Sleep mode freed 1.85 GiB memory, 8.80 GiB memory is still in use.\n",
            "INFO 08-12 10:03:34 [executor_base.py:210] It took 0.683286 seconds to fall asleep.\n",
            "INFO 08-12 10:03:56 [executor_base.py:226] It took 0.847544 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:03:56 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:03:56 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:03:56 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.50 GiB memory is still in use.\n",
            "INFO 08-12 10:03:56 [executor_base.py:210] It took 0.679676 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:04:13 [executor_base.py:226] It took 0.732770 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:04:53 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:04:53 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:04:53 [worker.py:145] Sleep mode freed 1.82 GiB memory, 8.84 GiB memory is still in use.\n",
            "INFO 08-12 10:04:53 [executor_base.py:210] It took 0.661362 seconds to fall asleep.\n",
            "INFO 08-12 10:05:07 [executor_base.py:226] It took 0.783884 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:05:07 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:05:07 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:05:08 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.23 GiB memory is still in use.\n",
            "INFO 08-12 10:05:08 [executor_base.py:210] It took 0.695577 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:05:16 [executor_base.py:226] It took 0.901709 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:06:31 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:06:31 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:06:32 [worker.py:145] Sleep mode freed 2.39 GiB memory, 8.81 GiB memory is still in use.\n",
            "INFO 08-12 10:06:32 [executor_base.py:210] It took 0.759822 seconds to fall asleep.\n",
            "INFO 08-12 10:06:52 [executor_base.py:226] It took 0.876165 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:06:52 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:06:52 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:06:53 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.61 GiB memory is still in use.\n",
            "INFO 08-12 10:06:53 [executor_base.py:210] It took 0.723174 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:07:07 [executor_base.py:226] It took 0.996918 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "********************Question:\n",
            "Letters $A, B, C,$ and $D$ represent four different digits from 0, 1, 2, 3, ..., 9. If $\\frac{A+B}{C+D}$ is an integer that is as large as possible, what is the value of $A+B$? \n",
            "Answer:\n",
            "17 \n",
            "Response:\n",
            "1\n",
            "We are given the equation:\n",
            "\n",
            "$$\n",
            "\\frac{A+B}{C+D} = \\begin{cases}\n",
            "\\text{integer} \\\\\n",
            "\\text{maximized}\n",
            "\\end{cases}\n",
            "$$\n",
            "\n",
            "where $A, B, C,$ and $D$ are four different digits from 0 to 9.\n",
            "\n",
            "Since $A, B, C,$ and $D$ are different, they must choose digits from 0 to 9 inclusive.\n",
            "\n",
            "We want to maximize the ratio, so let's start by choosing the largest possible $A$ and $B$. Since the digits are different,\n",
            "\n",
            "$$\n",
            "A = 9 \\quad \\text{and} \\quad B = 8\n",
            "$$\n",
            "\n",
            "This gives $A+B = 9+8 = 17$. Now, for $C$ and $D$, we want to minimize $C+D$ to maximize the ratio. Therefore, the smaller digits must be chosen for $C$ and $D$. We have 6 remaining digits (0, 1, 2, 3, 4, 5, 7), and we want to pick one of these two to be the smaller digits. So,\n",
            "\n",
            "$$\n",
            "C = 0 \\quad \\text{and} \\quad D = 1\n",
            "$$\n",
            "\n",
            "Now,\n",
            "\n",
            "$$\n",
            "C+D = 0 + 1 = 1\n",
            "$$\n",
            "\n",
            "The final ratio is:\n",
            "\n",
            "$$\n",
            "\\frac{9+8}{0+1} = 17\n",
            "$$\n",
            "\n",
            "<end_working_out>The value of $A+B$ is $\\boxed{17}$.<SOLUTION>17</SOLUTION> \n",
            "Extracted:\n",
            "17\n",
            "INFO 08-12 10:08:29 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:08:29 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:08:29 [worker.py:145] Sleep mode freed 1.81 GiB memory, 8.84 GiB memory is still in use.\n",
            "INFO 08-12 10:08:29 [executor_base.py:210] It took 0.726092 seconds to fall asleep.\n",
            "INFO 08-12 10:08:50 [executor_base.py:226] It took 0.896134 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:08:50 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:08:50 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:08:51 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.36 GiB memory is still in use.\n",
            "INFO 08-12 10:08:51 [executor_base.py:210] It took 0.705721 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:09:06 [executor_base.py:226] It took 0.843620 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:09:45 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:09:45 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:09:45 [worker.py:145] Sleep mode freed 2.38 GiB memory, 8.82 GiB memory is still in use.\n",
            "INFO 08-12 10:09:45 [executor_base.py:210] It took 0.795292 seconds to fall asleep.\n",
            "INFO 08-12 10:09:59 [executor_base.py:226] It took 0.953190 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:09:59 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:09:59 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:10:00 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.22 GiB memory is still in use.\n",
            "INFO 08-12 10:10:00 [executor_base.py:210] It took 0.919551 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:10:08 [executor_base.py:226] It took 0.761752 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:11:41 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:11:41 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:11:42 [worker.py:145] Sleep mode freed 2.40 GiB memory, 8.85 GiB memory is still in use.\n",
            "INFO 08-12 10:11:42 [executor_base.py:210] It took 0.774203 seconds to fall asleep.\n",
            "INFO 08-12 10:12:06 [executor_base.py:226] It took 0.974066 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:12:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:12:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:12:07 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.56 GiB memory is still in use.\n",
            "INFO 08-12 10:12:07 [executor_base.py:210] It took 0.723091 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:12:25 [executor_base.py:226] It took 0.840600 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:13:10 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:13:10 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:13:11 [worker.py:145] Sleep mode freed 2.42 GiB memory, 8.83 GiB memory is still in use.\n",
            "INFO 08-12 10:13:11 [executor_base.py:210] It took 0.754982 seconds to fall asleep.\n",
            "INFO 08-12 10:13:26 [executor_base.py:226] It took 0.898638 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:13:26 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:13:26 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:13:27 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.24 GiB memory is still in use.\n",
            "INFO 08-12 10:13:27 [executor_base.py:210] It took 0.910741 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:13:37 [executor_base.py:226] It took 0.815271 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:14:59 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:14:59 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:15:00 [worker.py:145] Sleep mode freed 2.35 GiB memory, 8.85 GiB memory is still in use.\n",
            "INFO 08-12 10:15:00 [executor_base.py:210] It took 0.795952 seconds to fall asleep.\n",
            "INFO 08-12 10:15:21 [executor_base.py:226] It took 0.936574 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:15:21 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:15:21 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:15:22 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.65 GiB memory is still in use.\n",
            "INFO 08-12 10:15:22 [executor_base.py:210] It took 0.751132 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:15:38 [executor_base.py:226] It took 0.848234 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "********************Question:\n",
            "How many distinct ordered pairs of positive integers $(m,n)$ are there so that the sum of the reciprocals of $m$ and $n$ is $\\frac14$? \n",
            "Answer:\n",
            "5 \n",
            "Response:\n",
            " To find the number of distinct ordered pairs $(m,n)$ of positive integers that satisfy the equation $\\frac{1}{m} + \\frac{1}{n} = \\frac{1}{4}$, we can start by setting a common denominator:\n",
            "\n",
            "$$ \\frac{1}{m} + \\frac{1}{n} = \\frac{m + n}{mn} = \\frac{1}{4}.$$\n",
            "\n",
            "Then, cross-multiply to get:\n",
            "\n",
            "$$4(m + n) = mn.$$\n",
            "\n",
            "Rearranging, we get a quadratic equation in terms of $m$:\n",
            "\n",
            "$$mn - 4m - 4n = 0.$$\n",
            "\n",
            "Let's now add 16 to both sides to complete the square:\n",
            "\n",
            "$$mn - 4m - 4n + 16 = 16.$$\n",
            "\n",
            "The left-hand side can be factored as:\n",
            "\n",
            "$$(m - 4)(n - 4) = 16.$$\n",
            "\n",
            "Given that $m$ and $n$ must be positive integers, the possible pairs $(m - 4, n - 4)$ that multiply to 16 are $(1, 16)$, $(2, 8)$, $(4, 4)$, $(8, 2)$, and $(16, 1)$. So the possible ordered pairs $(m,n)$ are:\n",
            "\n",
            "$$(5, 20), (6, 12), (8, 8), (12, 6), (20, 5)$$\n",
            "\n",
            "This produces 5 distinct ordered pairs. <end_working_out>\n",
            "\n",
            "Therefore, between <SOLUTION></SOLUTION>, the number of distinct ordered pairs of positive integers $(m,n)$ is **5**. </document> \n",
            "Extracted:\n",
            ",\n",
            "INFO 08-12 10:16:29 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:16:29 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:16:30 [worker.py:145] Sleep mode freed 1.82 GiB memory, 8.84 GiB memory is still in use.\n",
            "INFO 08-12 10:16:30 [executor_base.py:210] It took 0.833188 seconds to fall asleep.\n",
            "INFO 08-12 10:16:45 [executor_base.py:226] It took 0.938842 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:16:45 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:16:45 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:16:46 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.29 GiB memory is still in use.\n",
            "INFO 08-12 10:16:46 [executor_base.py:210] It took 0.753117 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:16:55 [executor_base.py:226] It took 1.000364 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:17:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:17:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:17:50 [worker.py:145] Sleep mode freed 2.35 GiB memory, 8.85 GiB memory is still in use.\n",
            "INFO 08-12 10:17:50 [executor_base.py:210] It took 0.842019 seconds to fall asleep.\n",
            "INFO 08-12 10:18:06 [executor_base.py:226] It took 0.913089 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:18:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:18:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:18:07 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.27 GiB memory is still in use.\n",
            "INFO 08-12 10:18:07 [executor_base.py:210] It took 0.778738 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:18:17 [executor_base.py:226] It took 0.821371 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:19:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:19:49 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:19:49 [worker.py:145] Sleep mode freed 1.82 GiB memory, 8.84 GiB memory is still in use.\n",
            "INFO 08-12 10:19:49 [executor_base.py:210] It took 0.811694 seconds to fall asleep.\n",
            "INFO 08-12 10:20:13 [executor_base.py:226] It took 1.186272 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:20:13 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:20:13 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:20:14 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.52 GiB memory is still in use.\n",
            "INFO 08-12 10:20:14 [executor_base.py:210] It took 0.796172 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:20:31 [executor_base.py:226] It took 0.864867 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:22:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:22:06 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:22:07 [worker.py:145] Sleep mode freed 2.35 GiB memory, 8.85 GiB memory is still in use.\n",
            "INFO 08-12 10:22:07 [executor_base.py:210] It took 0.876671 seconds to fall asleep.\n",
            "INFO 08-12 10:22:31 [executor_base.py:226] It took 1.008472 seconds to wake up tags {'weights', 'kv_cache'}.\n",
            "INFO 08-12 10:22:31 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:22:31 [prefix_caching_block.py:479] Successfully reset prefix cache\n",
            "INFO 08-12 10:22:32 [worker.py:145] Sleep mode freed 1.69 GiB memory, 9.57 GiB memory is still in use.\n",
            "INFO 08-12 10:22:32 [executor_base.py:210] It took 0.812379 seconds to fall asleep.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:unsloth_zoo.temporary_patches.common:Unsloth: num_items_in_batch = None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 08-12 10:22:50 [executor_base.py:226] It took 0.828303 seconds to wake up tags {'weights', 'kv_cache'}.\n"
          ]
        }
      ],
      "source": [
        "# For optional training + evaluation\n",
        "# new_dataset = dataset.train_test_split(test_size = 0.01)\n",
        "\n",
        "trainer = GRPOTrainer(\n",
        "    model = model,\n",
        "    processing_class = tokenizer,\n",
        "    reward_funcs = [\n",
        "        match_format_exactly,\n",
        "        match_format_approximately,\n",
        "        check_answer,\n",
        "        check_numbers,\n",
        "    ],\n",
        "    args = training_args,\n",
        "    train_dataset = dataset,\n",
        "\n",
        "    # For optional training + evaluation\n",
        "    # train_dataset = new_dataset[\"train\"],\n",
        "    # eval_dataset = new_dataset[\"test\"],\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlaUdxC_VHpz"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Now let's try the model we just trained! First, let's first try the model without any GRPO trained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcz_lpbVC92"
      },
      "outputs": [],
      "source": [
        "text = \"What is the sqrt of 101?\"\n",
        "\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 1024,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    [text],\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = None,\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Colxz9TAVMsi"
      },
      "source": [
        "And now with the LoRA we just trained with GRPO - we first save the LoRA first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL-BcuB1VLIv"
      },
      "outputs": [],
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4LMOBl8boGX"
      },
      "source": [
        "Verify LoRA is actually trained!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SfdI-ERbpiw"
      },
      "outputs": [],
      "source": [
        "from safetensors import safe_open\n",
        "\n",
        "tensors = {}\n",
        "with safe_open(\"grpo_saved_lora/adapter_model.safetensors\", framework = \"pt\") as f:\n",
        "    # Verify both A and B are non zero\n",
        "    for key in f.keys():\n",
        "        tensor = f.get_tensor(key)\n",
        "        n_zeros = (tensor == 0).sum() / tensor.numel()\n",
        "        assert(n_zeros.item() != tensor.numel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwpbwnDBVRLg"
      },
      "source": [
        "Now we load the LoRA and test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf_OY5WMVOxF"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the sqrt of 101?\"},\n",
        "]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    tokenize = False,\n",
        ")\n",
        "from vllm import SamplingParams\n",
        "sampling_params = SamplingParams(\n",
        "    temperature = 1.0,\n",
        "    top_k = 50,\n",
        "    max_tokens = 2048,\n",
        ")\n",
        "output = model.fast_generate(\n",
        "    text,\n",
        "    sampling_params = sampling_params,\n",
        "    lora_request = model.load_lora(\"grpo_saved_lora\"),\n",
        ")[0].outputs[0].text\n",
        "\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aDgFfhFYIAS"
      },
      "source": [
        "Our reasoning model is much better - it's not always correct, since we only trained it for an hour or so - it'll be better if we extend the sequence length and train for longer!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NUEmHFSYNTp"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjXGTkp7YNtB"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
        "\n",
        "# Merge to 4bit\n",
        "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
        "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
        "\n",
        "# Just LoRA adapters\n",
        "if False:\n",
        "    model.save_pretrained(\"model\")\n",
        "    tokenizer.save_pretrained(\"model\")\n",
        "if False:\n",
        "    model.push_to_hub(\"hf/model\", token = \"\")\n",
        "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52WMb3k_YPt8"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "[**NEW**] To finetune and auto export to Ollama, try our [Ollama notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyEjW-WuYQIm"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V15Yhj1V9lwG"
      },
      "source": [
        "Now, use the `model-unsloth.gguf` file or `model-unsloth-Q4_K_M.gguf` file in llama.cpp or a UI based system like Jan or Open WebUI. You can install Jan [here](https://github.com/janhq/jan) and Open WebUI [here](https://github.com/open-webui/open-webui)\n",
        "\n",
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Some other links:\n",
        "1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n",
        "2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n",
        "3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n",
        "6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n",
        "  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n",
        "\n",
        "  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n",
        "</div>\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c58a41cfa9644288fe491207553d51b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_66c184cf05b548ed8a6e73e598ebc622",
              "IPY_MODEL_d824bb0eca3e443fa5d0fdfb12a9dc29",
              "IPY_MODEL_a838ff62428d478aad5c7a39886cea26"
            ],
            "layout": "IPY_MODEL_ec3a12bc77fb47159c982e4212c31bf3"
          }
        },
        "66c184cf05b548ed8a6e73e598ebc622": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92dadac30d0f4aa1989dd787b2e1eea1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7695b5c82c2348c0bf4efd63cc781cdc",
            "value": ""
          }
        },
        "d824bb0eca3e443fa5d0fdfb12a9dc29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2255ebe61a5f4a759a3ee52359c95f87",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c95999c10ba4f5ab3401dfb3fb6764d",
            "value": 2
          }
        },
        "a838ff62428d478aad5c7a39886cea26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88161b307fb448ed90905533ff1a9069",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_442dce254cb34ad8a1bf21e74a464b3c",
            "value": "Loading‚Äásafetensors‚Äácheckpoint‚Äáshards:‚Äá100%‚ÄáCompleted‚Äá|‚Äá2/2‚Äá[00:37&lt;00:00,‚Äá19.74s/it]\n"
          }
        },
        "ec3a12bc77fb47159c982e4212c31bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92dadac30d0f4aa1989dd787b2e1eea1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7695b5c82c2348c0bf4efd63cc781cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2255ebe61a5f4a759a3ee52359c95f87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c95999c10ba4f5ab3401dfb3fb6764d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88161b307fb448ed90905533ff1a9069": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "442dce254cb34ad8a1bf21e74a464b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59a7fd28ac8d45108fac4d589fd35b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72bd93276c904ad2bdcb985eae28b472",
              "IPY_MODEL_9a3a27e099d04c0daa4c289f64fc11f2",
              "IPY_MODEL_d3409807f3a14e49adc56e64a9fc5760"
            ],
            "layout": "IPY_MODEL_847fb8ea629f494dbdab8174193723c0"
          }
        },
        "72bd93276c904ad2bdcb985eae28b472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11281f513dc44571af9e0c2afc65154e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ee41c56e13f74c04be6f17048b5f66e2",
            "value": "Capturing‚ÄáCUDA‚Äágraph‚Äáshapes:‚Äá100%"
          }
        },
        "9a3a27e099d04c0daa4c289f64fc11f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83149feab93b4a348a263573be63ec53",
            "max": 23,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04d35183b4344d12a6d1c7748acbe3db",
            "value": 23
          }
        },
        "d3409807f3a14e49adc56e64a9fc5760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cd416da687c24ba89f944d726b60dcda",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0e8d4212221542ee91422a40a804e529",
            "value": "‚Äá23/23‚Äá[00:16&lt;00:00,‚Äá‚Äá1.35s/it]"
          }
        },
        "847fb8ea629f494dbdab8174193723c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11281f513dc44571af9e0c2afc65154e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee41c56e13f74c04be6f17048b5f66e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83149feab93b4a348a263573be63ec53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04d35183b4344d12a6d1c7748acbe3db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cd416da687c24ba89f944d726b60dcda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e8d4212221542ee91422a40a804e529": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41033ac99c584df3b1709fe6d5f49931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8163e2ad75a49a1aff0fb3b51678a89",
              "IPY_MODEL_d13156aa0c134cbf8ade77a6dfc0c522",
              "IPY_MODEL_ddd758d7c4e3449480cce8dd717ef4e2"
            ],
            "layout": "IPY_MODEL_1e5896313b854b94a94e939b637bc243"
          }
        },
        "f8163e2ad75a49a1aff0fb3b51678a89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5e1287495354659afb8ceea3bb423ed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a8d507942f4048c1b6c45aa4168a90f0",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "d13156aa0c134cbf8ade77a6dfc0c522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80b296b2e4b744f598dec6ddbc368176",
            "max": 59,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85f525f32a9a465e8103df82a00aac51",
            "value": 59
          }
        },
        "ddd758d7c4e3449480cce8dd717ef4e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5347a8f55d3b49fd859dfaa94c25372e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2a96d0ad9dd740ffbafd946782544a29",
            "value": "‚Äá59/59‚Äá[00:01&lt;00:00,‚Äá18.96‚Äáexamples/s]"
          }
        },
        "1e5896313b854b94a94e939b637bc243": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5e1287495354659afb8ceea3bb423ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8d507942f4048c1b6c45aa4168a90f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80b296b2e4b744f598dec6ddbc368176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85f525f32a9a465e8103df82a00aac51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5347a8f55d3b49fd859dfaa94c25372e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a96d0ad9dd740ffbafd946782544a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}